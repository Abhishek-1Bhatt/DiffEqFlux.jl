var documenterSearchIndex = {"docs":
[{"location":"Benchmark/#Benchmarks-1","page":"Benchmark","title":"Benchmarks","text":"","category":"section"},{"location":"Benchmark/#Vs-Torchdiffeq-on-small-ODEs-1","page":"Benchmark","title":"Vs Torchdiffeq on small ODEs","text":"","category":"section"},{"location":"Benchmark/#","page":"Benchmark","title":"Benchmark","text":"A raw ODE solver benchmark showcases a 30,000x performance advantage over torchdiffeq on small ODEs.","category":"page"},{"location":"Benchmark/#A-bunch-of-adjoint-choices-on-neural-ODEs-1","page":"Benchmark","title":"A bunch of adjoint choices on neural ODEs","text":"","category":"section"},{"location":"Benchmark/#","page":"Benchmark","title":"Benchmark","text":"Quick summary:","category":"page"},{"location":"Benchmark/#","page":"Benchmark","title":"Benchmark","text":"BacksolveAdjoint is the fastest (but use with caution!); about 25% faster\nUsing ZygoteVJP is faster than other vjp choices with FastDense due to the overloads","category":"page"},{"location":"Benchmark/#","page":"Benchmark","title":"Benchmark","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, DiffEqSensitivity,\n      Zygote, BenchmarkTools, Random\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nRandom.seed!(100)\np = initial_params(dudt2)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction loss_neuralode(p)\n    pred = Array(prob_neuralode(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode,p)\n# 2.709 ms (56506 allocations: 6.62 MiB)\n\nprob_neuralode_interpolating = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_interpolating(p)\n    pred = Array(prob_neuralode_interpolating(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_interpolating,p)\n# 5.501 ms (103835 allocations: 2.57 MiB)\n\nprob_neuralode_interpolating_zygote = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()))\n\nfunction loss_neuralode_interpolating_zygote(p)\n    pred = Array(prob_neuralode_interpolating_zygote(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_interpolating_zygote,p)\n# 2.899 ms (56150 allocations: 6.61 MiB)\n\nprob_neuralode_backsolve = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_backsolve(p)\n    pred = Array(prob_neuralode_backsolve(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve,p)\n# 4.871 ms (85855 allocations: 2.20 MiB)\n\nprob_neuralode_quad = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_quad(p)\n    pred = Array(prob_neuralode_quad(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_quad,p)\n# 11.748 ms (79549 allocations: 3.87 MiB)\n\nprob_neuralode_backsolve_tracker = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=TrackerVJP()))\n\nfunction loss_neuralode_backsolve_tracker(p)\n    pred = Array(prob_neuralode_backsolve_tracker(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_tracker,p)\n# 27.604 ms (186143 allocations: 12.22 MiB)\n\nprob_neuralode_backsolve_zygote = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ZygoteVJP()))\n\nfunction loss_neuralode_backsolve_zygote(p)\n    pred = Array(prob_neuralode_backsolve_zygote(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_zygote,p)\n# 2.091 ms (49883 allocations: 6.28 MiB)\n\nprob_neuralode_backsolve_false = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP(false)))\n\nfunction loss_neuralode_backsolve_false(p)\n    pred = Array(prob_neuralode_backsolve_false(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_false,p)\n# 4.822 ms (9956 allocations: 1.03 MiB)\n\nprob_neuralode_tracker = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=TrackerAdjoint())\n\nfunction loss_neuralode_tracker(p)\n    pred = Array(prob_neuralode_tracker(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_tracker,p)\n# 12.614 ms (76346 allocations: 3.12 MiB)","category":"page"},{"location":"tensor_layer/#Tensor-Product-Layer-1","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"","category":"section"},{"location":"tensor_layer/#","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"The following layer is a helper function for easily constructing a TensorLayer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn], a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].","category":"page"},{"location":"tensor_layer/#","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"TensorLayer","category":"page"},{"location":"tensor_layer/#DiffEqFlux.TensorLayer","page":"Tensor Product Layer","title":"DiffEqFlux.TensorLayer","text":"Constructs the Tensor Product Layer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn] a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].\n\nTensorLayer(model,out,p=nothing)\n\nArguments:\n\nmodel: Array of TensorProductBasis [B1(n1), ..., Bk(nk)], where k corresponds to the dimension of the input.\nout: Dimension of the output.\np: Optional initialization of the layer's weight. Initizalized to 0 by default.\n\n\n\n\n\n","category":"type"},{"location":"examples/pde_constrained/#Partial-Differential-Equation-Constrained-Optimization-1","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"This example uses a prediction model to optimize the one-dimensional Burgers' Equation. (Step-by-step description below)","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"using DelimitedFiles,Plots\nusing DiffEqSensitivity, OrdinaryDiffEq, Zygote, Flux, DiffEqFlux, Optim\n\n# Problem setup parameters:\nLx = 10.0\nx  = 0.0:0.01:Lx\ndx = x[2] - x[1]\nNx = size(x)\n\nu0 = exp.(-(x.-3.0).^2) # I.C\n\n## Problem Parameters\np        = [1.0,1.0]    # True solution parameters\nxtrs     = [dx,Nx]      # Extra parameters\ndt       = 0.40*dx^2    # CFL condition\nt0, tMax = 0.0 ,1000*dt\ntspan    = (t0,tMax)\nt        = t0:dt:tMax;\n\n## Definition of Auxiliary functions\nfunction ddx(u,dx)\n    \"\"\"\n    2nd order Central difference for 1st degree derivative\n    \"\"\"\n    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]\nend\n\n\nfunction d2dx(u,dx)\n    \"\"\"\n    2nd order Central difference for 2nd degree derivative\n    \"\"\"\n    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]\nend\n\n## ODE description of the Physics:\nfunction burgers(u,p,t)\n    # Model parameters\n    a0, a1 = p\n    dx,Nx = xtrs #[1.0,3.0,0.125,100]\n    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)\nend\n\n# Testing Solver on linear PDE\nprob = ODEProblem(burgers,u0,tspan,p)\nsol = solve(prob,Tsit5(), dt=dt,saveat=t);\n\nplot(x, sol.u[1], lw=3, label=\"t0\", size=(800,500))\nplot!(x, sol.u[end],lw=3, ls=:dash, label=\"tMax\")\n\nps  = [0.1, 0.2];   # Initial guess for model parameters\nfunction predict(θ)\n    Array(solve(prob,Tsit5(),p=θ,dt=dt,saveat=t))\nend\n\n## Defining Loss function\nfunction loss(θ)\n    pred = predict(θ)\n    l = predict(θ)  - sol\n    return sum(abs2, l), pred # Mean squared error\nend\n\nl,pred   = loss(ps)\nsize(pred), size(sol), size(t) # Checking sizes\n\nLOSS  = []                              # Loss accumulator\nPRED  = []                              # prediction accumulator\nPARS  = []                              # parameters accumulator\n\ncb = function (θ,l,pred) #callback function to observe training\n  display(l)\n  append!(PRED, [pred])\n  append!(LOSS, l)\n  append!(PARS, [θ])\n  false\nend\n\ncb(ps,loss(ps)...) # Testing callback function\n\n# Let see prediction vs. Truth\nscatter(sol[:,end], label=\"Truth\", size=(800,500))\nplot!(PRED[end][:,end], lw=2, label=\"Prediction\")\n\nres = DiffEqFlux.sciml_train(loss, ps, ADAM(0.01), cb = cb, maxiters = 100)  # Let check gradient propagation\nps = res.minimizer\nres = DiffEqFlux.sciml_train(loss, ps, BFGS(), cb = cb, maxiters = 100)  # Let check gradient propagation\n@show res.minimizer # returns [0.999999999999975, 1.0000000000000213]","category":"page"},{"location":"examples/pde_constrained/#Step-by-step-Description-1","page":"Partial Differential Equation Constrained Optimization","title":"Step-by-step Description","text":"","category":"section"},{"location":"examples/pde_constrained/#Load-Packages-1","page":"Partial Differential Equation Constrained Optimization","title":"Load Packages","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"using DelimitedFiles,Plots\nusing DiffEqSensitivity, OrdinaryDiffEq, Zygote, Flux, DiffEqFlux, Optim","category":"page"},{"location":"examples/pde_constrained/#Parameters-1","page":"Partial Differential Equation Constrained Optimization","title":"Parameters","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"First, we setup the 1-dimensional space over which our equations will be evaluated. x spans from 0.0 to 10.0 in steps of 0.01; t spans from 0.00 to 0.04 in steps of 4.0e-5.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"# Problem setup parameters:\nLx = 10.0\nx  = 0.0:0.01:Lx\ndx = x[2] - x[1]\nNx = size(x)\n\nu0 = exp.(-(x.-3.0).^2) # I.C\n\n## Problem Parameters\np        = [1.0,1.0]    # True solution parameters\nxtrs     = [dx,Nx]      # Extra parameters\ndt       = 0.40*dx^2    # CFL condition\nt0, tMax = 0.0 ,1000*dt\ntspan    = (t0,tMax)\nt        = t0:dt:tMax;","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"In plain terms, the quantities that were defined are:","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"x (to Lx) spans the specified 1D space\ndx = distance between two points\nNx = total size of space\nu0 = initial condition\np = true solution\nxtrs = convenient grouping of dx and Nx into Array\ndt = time distane between two points\nt (t0 to tMax) spans the specified time frame\ntspan = span of t","category":"page"},{"location":"examples/pde_constrained/#Auxiliary-Functions-1","page":"Partial Differential Equation Constrained Optimization","title":"Auxiliary Functions","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"We then define two functions to compute the derivatives numerically. The Central Difference is used in both the 1st and 2nd degree derivatives.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"## Definition of Auxiliary functions\nfunction ddx(u,dx)\n    \"\"\"\n    2nd order Central difference for 1st degree derivative\n    \"\"\"\n    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]\nend\n\n\nfunction d2dx(u,dx)\n    \"\"\"\n    2nd order Central difference for 2nd degree derivative\n    \"\"\"\n    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]\nend","category":"page"},{"location":"examples/pde_constrained/#Burgers'-Differential-Equation-1","page":"Partial Differential Equation Constrained Optimization","title":"Burgers' Differential Equation","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"Next, we setup our desired set of equations in order to define our problem.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"## ODE description of the Physics:\nfunction burgers(u,p,t)\n    # Model parameters\n    a0, a1 = p\n    dx,Nx = xtrs #[1.0,3.0,0.125,100]\n    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)\nend","category":"page"},{"location":"examples/pde_constrained/#Solve-and-Plot-Ground-Truth-1","page":"Partial Differential Equation Constrained Optimization","title":"Solve and Plot Ground Truth","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"We then solve and plot our partial differential equation. This is the true solution which we will compare to further on.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"# Testing Solver on linear PDE\nprob = ODEProblem(burgers,u0,tspan,p)\nsol = solve(prob,Tsit5(), dt=dt,saveat=t);\n\nplot(x, sol.u[1], lw=3, label=\"t0\", size=(800,500))\nplot!(x, sol.u[end],lw=3, ls=:dash, label=\"tMax\")","category":"page"},{"location":"examples/pde_constrained/#Building-the-Prediction-Model-1","page":"Partial Differential Equation Constrained Optimization","title":"Building the Prediction Model","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"Now we start building our prediction model to try to obtain the values p. We make an initial guess for the parameters and name it ps here. The predict function is a non-linear transformation in one layer using solve. If unfamiliar with the concept, refer to here.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"ps  = [0.1, 0.2];   # Initial guess for model parameters\nfunction predict(θ)\n    Array(solve(prob,Tsit5(),p=θ,dt=dt,saveat=t))\nend","category":"page"},{"location":"examples/pde_constrained/#Train-Parameters-1","page":"Partial Differential Equation Constrained Optimization","title":"Train Parameters","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"Training our model requires a loss function, an optimizer and a callback function to display the progress.","category":"page"},{"location":"examples/pde_constrained/#Loss-1","page":"Partial Differential Equation Constrained Optimization","title":"Loss","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"We first make our predictions based on the current values of our parameters ps, then take the difference between the predicted solution and the truth above. For the loss, we use the Mean squared error.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"## Defining Loss function\nfunction loss(θ)\n    pred = predict(θ)\n    l = predict(θ)  - sol\n    return sum(abs2, l), pred # Mean squared error\nend\n\nl,pred   = loss(ps)\nsize(pred), size(sol), size(t) # Checking sizes","category":"page"},{"location":"examples/pde_constrained/#Optimizer-1","page":"Partial Differential Equation Constrained Optimization","title":"Optimizer","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"The optimizers ADAM with a learning rate of 0.01 and BFGS are directly passed in training (see below)","category":"page"},{"location":"examples/pde_constrained/#Callback-1","page":"Partial Differential Equation Constrained Optimization","title":"Callback","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"The callback function displays the loss during training. We also keep a history of the loss, the previous predictions and the previous parameters with LOSS, PRED and PARS accumulators.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"LOSS  = []                              # Loss accumulator\nPRED  = []                              # prediction accumulator\nPARS  = []                              # parameters accumulator\n\ncb = function (θ,l,pred) #callback function to observe training\n  display(l)\n  append!(PRED, [pred])\n  append!(LOSS, l)\n  append!(PARS, [θ])\n  false\nend\n\ncb(ps,loss(ps)...) # Testing callback function","category":"page"},{"location":"examples/pde_constrained/#Plotting-Prediction-vs-Ground-Truth-1","page":"Partial Differential Equation Constrained Optimization","title":"Plotting Prediction vs Ground Truth","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"The scatter points plotted here are the ground truth obtained from the actual solution we solved for above. The solid line represents our prediction. The goal is for both to overlap almost perfectly when the PDE finishes its training and the loss is close to 0.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"# Let see prediction vs. Truth\nscatter(sol[:,end], label=\"Truth\", size=(800,500))\nplot!(PRED[end][:,end], lw=2, label=\"Prediction\")","category":"page"},{"location":"examples/pde_constrained/#Train-1","page":"Partial Differential Equation Constrained Optimization","title":"Train","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"The parameters are trained using sciml_train and adjoint sensitivities. The resulting best parameters are stored in res and res.minimizer returns the parameters that minimizes the cost function.","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"res = DiffEqFlux.sciml_train(loss, ps, ADAM(0.01), cb = cb, maxiters = 100)  # Let check gradient propagation\nps = res.minimizer\nres = DiffEqFlux.sciml_train(loss, ps, BFGS(), cb = cb, maxiters = 100)  # Let check gradient propagation\n@show res.minimizer # returns [0.999999999999975, 1.0000000000000213]","category":"page"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"We successfully predict the final ps to be equal to [0.999999999999975, 1.0000000000000213] vs the true solution of p = [1.0, 1.0]","category":"page"},{"location":"examples/pde_constrained/#Expected-Output-1","page":"Partial Differential Equation Constrained Optimization","title":"Expected Output","text":"","category":"section"},{"location":"examples/pde_constrained/#","page":"Partial Differential Equation Constrained Optimization","title":"Partial Differential Equation Constrained Optimization","text":"153.74716386883014\n153.74716386883014\n150.31001476832154\n146.91327105278128\n143.55759898759374\n140.24363496931753\n136.97198347241257\n133.7432151677673\n130.55786524987215\n127.4164319720337\n124.31937540894337\n121.26711645161134\n118.26003603654628\n115.29847461603427\n112.3827318609633\n109.51306659138356\n106.68969692777314\n103.9128006498965\n101.18251574195561\n98.4989411191655\n95.8621374998964\n93.27212842357801\n90.7289013677808\n88.23240896985287\n85.7825703121191\n83.37927225399383\n81.02237079935475\n78.71169247246975\n76.44703568540336\n74.22817209335733\n72.05484791455291\n69.92678520204167\n67.84368308185877\n65.80521891873633\n63.81104944163126\n61.860811797059554\n59.95412455791812\n58.090588663826914\n56.26978832428055\n54.491291863817686\n52.75465253618253\n51.05940929392087\n49.405087540342564\n47.79119984816457\n46.217246667009626\n44.68271701552145\n43.18708916553295\n41.729831330086824\n40.310402328506555\n38.928252289762675\n37.58282331100446\n36.27355015737786\n34.99986094007708\n33.76117780641769\n32.55691762379305\n31.386492661205562\n30.249311268822595\n29.144778544729924\n28.07229699202965\n27.031267166855155\n26.0210883069299\n25.041158938495613\n24.09087747422764\n23.169642780270983\n22.276854715336583\n21.411914664407295\n20.57422602075309\n19.76319467338999\n18.978229434706996\n18.218742481097735\n17.48414972880479\n16.773871221320032\n16.087331469276343\n15.423959781047255\n14.78319057598673\n14.164463661389682\n13.567224508247984\n12.990924508800399\n12.435021204904853\n11.898978515303417\n11.382266943971572\n10.884363779196345\n10.404753276294088\n9.942926832732251\n9.49838314770057\n9.070628379941386\n8.659176278010788\n8.263548334737965\n7.883273889583058\n7.517890250788576\n7.1669427976429585\n6.829985075319055\n6.506578881124348\n6.19629433688754\n5.898709957062298\n5.613412692266443\n5.339997993203038\n5.078069839645422\n4.827240754206443\n4.587131834698446\n4.357372763056912\n4.357372763056912\n4.137601774726927\n1.5254536025963588\n0.0023707487489687726\n4.933077457357198e-7\n8.157805551380282e-14\n1.6648677430325974e-16\nres.minimizer = [0.999999999999975, 1.0000000000000213]\n2-element Array{Float64,1}:\n 0.999999999999975\n 1.0000000000000213","category":"page"},{"location":"examples/second_order_adjoints/#Newton-and-Hessian-Free-Newton-Krylov-with-Second-Order-Adjoint-Sensitivity-Analysis-1","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"","category":"section"},{"location":"examples/second_order_adjoints/#","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"In many cases it may be more optimal or more stable to fit using second order Newton-based optimization techniques. Since DiffEqSensitivity.jl provides second order sensitivity analysis for fast Hessians and Hessian-vector products (via forward-over-reverse), we can utilize these in our neural/universal differential equation training processes.","category":"page"},{"location":"examples/second_order_adjoints/#","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"sciml_train is setup to automatically use second order sensitivity analysis methods if a second order optimizer is requested via Optim.jl. Thus Newton and NewtonTrustRegion optimizers will use a second order Hessian-based optimization, while KrylovTrustRegion will utilize a Krylov-based method with Hessian-vector products (never forming the Hessian) for large parameter optimizations.","category":"page"},{"location":"examples/second_order_adjoints/#","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.0; 0.0]\r\ndatasize = 30\r\ntspan = (0.0f0, 1.5f0)\r\ntsteps = range(tspan[1], tspan[2], length = datasize)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r\n\r\ndudt2 = FastChain((x, p) -> x.^3,\r\n                  FastDense(2, 50, tanh),\r\n                  FastDense(50, 2))\r\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\r\np = prob_neuralode.p\r\n\r\nfunction predict_neuralode(p)\r\n  Array(prob_neuralode(u0, p))\r\nend\r\n\r\nfunction loss_neuralode(p)\r\n    pred = predict_neuralode(p)\r\n    loss = sum(abs2, ode_data .- pred)\r\n    return loss, pred\r\nend\r\n\r\n# Callback function to observe training\r\nlist_plots = []\r\niter = 0\r\ncb = function (p, l, pred; doplot = false)\r\n  global list_plots, iter\r\n\r\n  if iter == 0\r\n    list_plots = []\r\n  end\r\n  iter += 1\r\n\r\n  display(l)\r\n\r\n  # plot current prediction against data\r\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\r\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\r\n  push!(list_plots, plt)\r\n  if doplot\r\n    display(plot(plt))\r\n  end\r\n\r\n  return l < 0.01\r\nend\r\n\r\npstart = DiffEqFlux.sciml_train(loss_neuralode, p, ADAM(0.01), cb=cb, maxiters = 100).minimizer\r\npmin = DiffEqFlux.sciml_train(loss_neuralode, pstart, cb=cb, NewtonTrustRegion())\r\npmin = DiffEqFlux.sciml_train(loss_neuralode, pstart, cb=cb, Optim.KrylovTrustRegion())","category":"page"},{"location":"examples/second_order_adjoints/#","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"Note that we do note demonstrate Newton() because we have not found a single case where it is competitive with the other two methods. KrylovTrustRegion() is generally the fastest due to its use of Hessian-vector products.","category":"page"},{"location":"examples/neural_ode_flux/#Neural-Ordinary-Differential-Equations-with-Flux.train!-1","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"","category":"section"},{"location":"examples/neural_ode_flux/#","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"The following is the same neural ODE example as before, but now using Flux.jl directly with Flux.train!. Notice that the only difference is that we have to make the neural network be a Chain and use Flux.jl's Flux.params implicit parameter system.","category":"page"},{"location":"examples/neural_ode_flux/#","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nfunction predict_n_ode()\r\n  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))\r\nend\r\n\r\nfunction loss_n_ode()\r\n    pred = predict_n_ode()\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss\r\nend\r\n\r\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\r\n\r\ncb = function (;doplot=false) #callback function to observe training\r\n  pred = predict_n_ode()\r\n  display(sum(abs2,ode_data .- pred))\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb()\r\n\r\ndata = Iterators.repeated((), 1000)\r\nFlux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = cb)","category":"page"},{"location":"examples/neural_ode_flux/#","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"(Image: )","category":"page"},{"location":"CNFLayer/#CNF-Layer-Functions-1","page":"Continuous Normalizing Flows Layer","title":"CNF Layer Functions","text":"","category":"section"},{"location":"CNFLayer/#","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"The following layer is a helper function for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows (CNF).","category":"page"},{"location":"CNFLayer/#","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"FFJORD","category":"page"},{"location":"CNFLayer/#DiffEqFlux.FFJORD","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORD","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2]. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z,θ,t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nNeuralODE(model,basedist=nothing,monte_carlo=false,tspan,args...;kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the ̇x.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\nmonte_carlo: Method for calculating the trace of the Jacobian. The default montecarlo = false calculates the Jacobian and its trace directly. montecarlo = true uses the stochastic approach presented in [3] to provide an unbiased estimate for the trace.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nRef [1]L. S. Pontryagin, Mathematical Theory of Optimal Processes. CRC Press, 1987. [2]R. T. Q. Chen, Y. Rubanova, J. Bettencourt, D. Duvenaud. Neural Ordinary Differential Equations. arXiv preprint at arXiv1806.07366, 2019. [3]W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, D. Duvenaud. FFJORD: Free-Form Continuous Dynamic For Scalable Reversible Generative Models. arXiv preprint at arXiv1810.01367, 2018.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#Neural-Differential-Equation-Layer-Functions-1","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layer Functions","text":"","category":"section"},{"location":"NeuralDELayers/#","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way. As demonstrated in the tutorials, they do not have to be used since automatic differentiation will just work over solve, but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type.","category":"page"},{"location":"NeuralDELayers/#","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"NeuralODE\nNeuralDSDE\nNeuralSDE\nNeuralCDDE\nNeuralDAE\nNeuralODEMM\nAugmentedNDELayer","category":"page"},{"location":"NeuralDELayers/#DiffEqFlux.NeuralODE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODE","text":"Constructs a continuous-time recurrant neural network, also known as a neural ordinary differential equation (neural ODE), with a fast gradient calculation via adjoints [1]. At a high level this corresponds to solving the forward differential equation, using a second differential equation that propagates the derivatives of the loss backwards in time.\n\nNeuralODE(model,tspan,alg=nothing,args...;kwargs...)\nNeuralODE(model::FastChain,tspan,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=DiffEqSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇x.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nRef [1]L. S. Pontryagin, Mathematical Theory of Optimal Processes. CRC Press, 1987.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#DiffEqFlux.NeuralDSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDSDE","text":"Constructs a neural stochastic differential equation (neural SDE) with diagonal noise.\n\nNeuralDSDE(model1,model2,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\nNeuralDSDE(model1::FastChain,model2::FastChain,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a vector of the same size as the input.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#DiffEqFlux.NeuralSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralSDE","text":"Constructs a neural stochastic differential equation (neural SDE).\n\nNeuralSDE(model1,model2,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralSDE(model1::FastChain,model2::FastChain,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a matrix that is nbrown x size(x,1).\ntspan: The timespan to be solved on.\nnbrown: The number of Brownian processes\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#DiffEqFlux.NeuralCDDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralCDDE","text":"Constructs a neural delay differential equation (neural DDE) with constant delays.\n\nNeuralCDDE(model,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralCDDE(model::FastChain,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size [x;x(t-lag_1);...;x(t-lag_n)] and produce and output shaped like x.\ntspan: The timespan to be solved on.\nhist: Defines the history function h(t) for values before the start of the integration.\nlags: Defines the lagged values that should be utilized in the neural network.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#DiffEqFlux.NeuralDAE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDAE","text":"Constructs a neural differential-algebraic equation (neural DAE).\n\nNeuralDAE(model,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralDAE(model::FastChain,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size x and produce the residual of f(dx,x,t) for only the differential variables.\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#DiffEqFlux.NeuralODEMM","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODEMM","text":"Constructs a physically-constrained continuous-time recurrant neural network, also known as a neural differential-algebraic equation (neural DAE), with a mass matrix and a fast gradient calculation via adjoints [1]. The mass matrix formulation is:\n\nMu = f(upt)\n\nwhere M is semi-explicit, i.e. singular with zeros for rows corresponding to the constraint equations.\n\nNeuralODEMM(model,constraints_model,tspan,alg=nothing,args...;kwargs...)\nNeuralODEMM(model::FastChain,tspan,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=DiffEqSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇f(u,p,t)\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl. This method requires an implicit ODE solver compatible with singular mass matrices. Consult the DAE solvers documentation for more details.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"NeuralDELayers/#DiffEqFlux.AugmentedNDELayer","page":"Neural Differential Equation Layers","title":"DiffEqFlux.AugmentedNDELayer","text":"Constructs an Augmented Neural Differential Equation Layer.\n\nAugmentedNDELayer(nde, adim::Int)\n\nArguments:\n\nnde: Any Neural Differential Equation Layer\nadim: The number of dimensions the initial conditions should be lifted\n\nReferences:\n\n[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural odes.\" Advances in Neural Information Processing Systems. 2019.\n\n\n\n\n\n","category":"type"},{"location":"examples/local_minima/#Strategies-to-Avoid-Local-Minima-1","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"","category":"section"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Local minima can be an issue with fitting neural differential equations. However, there are many strategies to avoid local minima:","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Insert stochasticity into the loss function through minibatching\nWeigh the loss function to allow for fitting earlier portions first\nIteratively grow the fit","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"In this example we will show how to use strategy (3) in order to increase the robustness of the fit. Let's start with the same neural ODE example we've used before except with one small twist: we wish to find the neural ODE that fits on (0,5.0). Naively, we use the same training strategy as before:","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.0; 0.0]\r\ndatasize = 30\r\ntspan = (0.0f0, 5.0f0)\r\ntsteps = range(tspan[1], tspan[2], length = datasize)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r\n\r\ndudt2 = FastChain((x, p) -> x.^3,\r\n                  FastDense(2, 16, tanh),\r\n                  FastDense(16, 2))\r\nprob_neuralode = NeuralODE(dudt2, tspan, Vern7(), saveat = tsteps, abstol=1e-6, reltol=1e-6)\r\n\r\nfunction predict_neuralode(p)\r\n  Array(prob_neuralode(u0, p))\r\nend\r\n\r\nfunction loss_neuralode(p)\r\n    pred = predict_neuralode(p)\r\n    loss = sum(abs2, (ode_data[:,1:size(pred,2)] .- pred))\r\n    return loss, pred\r\nend\r\n\r\niter = 0\r\ncallback = function (p, l, pred; doplot = true)\r\n  global iter\r\n  iter += 1\r\n\r\n  display(l)\r\n  if doplot\r\n    # plot current prediction against data\r\n    plt = scatter(tsteps[1:size(pred,2)], ode_data[1,1:size(pred,2)], label = \"data\")\r\n    scatter!(plt, tsteps[1:size(pred,2)], pred[1,:], label = \"prediction\")\r\n    display(plot(plt))\r\n  end\r\n\r\n  return false\r\nend\r\n\r\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\r\n                                          ADAM(0.05), cb = callback,\r\n                                          maxiters = 300)\r\n\r\ncallback(result_neuralode2.minimizer,loss_neuralode(result_neuralode.minimizer)...;doplot=true)\r\nsavefig(\"local_minima.png\")","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"However, we've now fallen into a trap of a local minima. If the optimizer changes the parameters so it dips early, it will increase the loss because there will be more error in the later parts of the time series. Thus it tends to just stay flat and never fit perfectly. This thus suggests strategies (2) and (3): do not allow the later parts of the time series to influence the fit until the later stages. Strategy (3) seems to be more robust, so this is what will be demonstrated.","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Let's start by reducing the timespan to (0,1.5):","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0,1.5), Tsit5(), saveat = tsteps[tsteps .<= 1.5])\r\n\r\nresult_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\r\n                                           ADAM(0.05), cb = callback,\r\n                                           maxiters = 300)\r\n\r\ncallback(result_neuralode2.minimizer,loss_neuralode(result_neuralode2.minimizer)...;doplot=true)\r\nsavefig(\"shortplot1.png\")","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"This fits beautifully. Now let's grow the timespan and utilize the parameters from our (0,1.5) fit as the initial condition to our next fit:","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0,3.0), Tsit5(), saveat = tsteps[tsteps .<= 3.0])\r\n\r\nresult_neuralode3 = DiffEqFlux.sciml_train(loss_neuralode,\r\n                                           result_neuralode2.minimizer,\r\n                                           ADAM(0.05), maxiters = 300,\r\n                                           cb = callback)\r\ncallback(result_neuralode3.minimizer,loss_neuralode(result_neuralode3.minimizer)...;doplot=true)\r\nsavefig(\"shortplot2.png\")","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Once again a great fit. Now we utilize these parameters as the initial condition to the full fit:","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0,5.0), Tsit5(), saveat = tsteps)\r\n\r\nresult_neuralode4 = DiffEqFlux.sciml_train(loss_neuralode,\r\n                                           result_neuralode3.minimizer,\r\n                                           ADAM(0.01), maxiters = 300,\r\n                                           cb = callback)\r\ncallback(result_neuralode4.minimizer,loss_neuralode(result_neuralode4.minimizer)...;doplot=true)\r\nsavefig(\"fullplot.png\")","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/#","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"And there we go, a robust strategy for fitting an equation that would otherwise get stuck in a local optima.","category":"page"},{"location":"examples/lotka_volterra/#Lotka-Volterra-with-Flux.train!-1","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"","category":"section"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"The following is a quick example of optimizing Lotka-Volterra parameters using the Flux.jl style. As before, we define the ODE that we want to solve:","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"using DiffEqFlux, DiffEqSensitivity, Flux, OrdinaryDiffEq, Zygote, Test #using Plots\r\n\r\nfunction lotka_volterra(du,u,p,t)\r\n  x, y = u\r\n  α, β, δ, γ = p\r\n  du[1] = dx = (α - β*y)x\r\n  du[2] = dy = (δ*x - γ)y\r\nend\r\np = [2.2, 1.0, 2.0, 0.4]\r\nu0 = [1.0,1.0]\r\nprob = ODEProblem(lotka_volterra,u0,(0.0,10.0),p)","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"Then we define our loss function with solve for the adjoint method:","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"function predict_rd()\r\n  Array(solve(prob,Tsit5(),saveat=0.1,reltol=1e-4))\r\nend\r\nloss_rd() = sum(abs2,x-1 for x in predict_rd())","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"Note that the parameters p to be optimized are not passed as arguments to the loss_rd or predict_rd functions.  This is because Flux.train! below uses implicit globals.","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"Now we setup the optimization. Here we choose the ADAM optimizer. To tell Flux what our parameters to optimize are, we use Flux.params(p). To make the optimizer run for 100 steps we use 100 outputs of blank data, i.e. Iterators.repeated((), 100) (this is where minibatching would go!).","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"opt = ADAM(0.1)\r\ncb = function ()\r\n  display(loss_rd())\r\n  #display(plot(solve(remake(prob,p=p),Tsit5(),saveat=0.1),ylim=(0,6)))\r\nend\r\n\r\n# Display the ODE with the current parameter values.\r\nFlux.train!(loss_rd, Flux.params(p), Iterators.repeated((), 100), opt, cb = cb)","category":"page"},{"location":"examples/lotka_volterra/#","page":"Lotka-Volterra with Flux.train!","title":"Lotka-Volterra with Flux.train!","text":"And now p will be the optimal parameter values for our chosen loss function.","category":"page"},{"location":"examples/optimization_sde/#Optimization-of-Stochastic-Differential-Equations-1","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"","category":"section"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Here we demonstrate sensealg = ForwardDiffSensitivity() (provided by DiffEqSensitivity.jl) for forward-mode automatic differentiation of a small stochastic differential equation. For large parameter equations, like neural stochastic differential equations, you should use reverse-mode automatic differentition. However, forward-mode can be more efficient for low numbers of parameters (<100).","category":"page"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using DifferentialEquations, Flux, Optim, DiffEqFlux, DiffEqSensitivity, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\nfunction lotka_volterra_noise!(du, u, p, t)\n  du[1] = 0.1u[1]\n  du[2] = 0.1u[2]\nend\n\nu0 = [1.0,1.0]\ntspan = (0.0, 10.0)\np = [2.2, 1.0, 2.0, 0.4]\nprob_sde = SDEProblem(lotka_volterra!, lotka_volterra_noise!, u0, tspan)\n\n\nfunction predict_sde(p)\n  return Array(solve(prob_sde, SOSRI(), p=p,\n               sensealg = ForwardDiffSensitivity(), saveat = 0.1))\nend\n\nloss_sde(p) = sum(abs2, x-1 for x in predict_sde(p))\nnothing","category":"page"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"For this training process, because the loss function is stochastic, we will use the ADAM optimizer from Flux.jl. The sciml_train function is the same as before. However, to speed up the training process, we will use a global counter so that way we only plot the current results every 10 iterations. This looks like:","category":"page"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"list_plots = []\niter = 0\ncallback = function (p, l)\n  global list_plots, iter\n\n  # List plots is reset to an empty list on the first callback\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  display(l)\n\n  if iter%10 == 1\n    remade_solution = solve(remake(prob_sde, p = p), SOSRI(), saveat = 0.1)\n    plt = plot(remade_solution, ylim = (0, 6))\n    push!(list_plots, plt)\n    display(plt)\n  end\n  return false\nend\nnothing # hide","category":"page"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's optimise","category":"page"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"result_sde = DiffEqFlux.sciml_train(loss_sde, p, ADAM(0.1),\n                                    cb = callback, maxiters = 100)","category":"page"},{"location":"examples/optimization_sde/#","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"Flux/#Use-with-Flux.jl-1","page":"Use with Flux Chain and train!","title":"Use with Flux.jl","text":"","category":"section"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"All of the tools of DiffEqFlux.jl can be used with Flux.jl. A lot of the examples have been written to use FastChain and sciml_train, but in all cases this can be changed to the Chain and Flux.train! workflow.","category":"page"},{"location":"Flux/#Using-Flux-Chain-neural-networks-with-Flux.train!-1","page":"Use with Flux Chain and train!","title":"Using Flux Chain neural networks with Flux.train!","text":"","category":"section"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"This should work almost automatically by using solve. Here is an example of optimizing u0 and p.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\ndudt(u,p,t) = dudt2(u)\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nfunction predict_n_ode()\r\n  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))\r\nend\r\n\r\nfunction loss_n_ode()\r\n    pred = predict_n_ode()\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss\r\nend\r\n\r\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\r\n\r\ncb = function (;doplot=false) #callback function to observe training\r\n  pred = predict_n_ode()\r\n  display(sum(abs2,ode_data .- pred))\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb()\r\n\r\ndata = Iterators.repeated((), 1000)\r\nres1 = Flux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = cb)","category":"page"},{"location":"Flux/#Using-Flux-Chain-neural-networks-with-sciml_train-1","page":"Use with Flux Chain and train!","title":"Using Flux Chain neural networks with sciml_train","text":"","category":"section"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"While for simple neural networks we recommend using FastChain-based neural networks for speed and simplicity, Flux neural networks can be used with sciml_train by utilizing the Flux.destructure function. In this case, if dudt is a Flux chain, then:","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"p,re = Flux.destructure(chain)","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"returns p which is the vector of parameters for the chain and re which is a function re(p) that reconstructs the neural network with new parameters p. Using this function we can thus build our neural differential equations in an explicit parameter style. For example, the neural ordinary differential equation example written out without using the NeuralODE helper would look like. Notice that in this example we will optimize both the neural network parameters p and the input initial condition u0. Notice that sciml_train works on a vector input, so we have to concatenate u0 and p and then in the loss function split to the pieces.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nθ = [u0;p] # the parameter vector to optimize\r\n\r\nfunction predict_n_ode(θ)\r\n  Array(solve(prob,Tsit5(),u0=θ[1:2],p=θ[3:end],saveat=t))\r\nend\r\n\r\nfunction loss_n_ode(θ)\r\n    pred = predict_n_ode(θ)\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss,pred\r\nend\r\n\r\nloss_n_ode(θ)\r\n\r\ncb = function (θ,l,pred;doplot=false) #callback function to observe training\r\n  display(l)\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb(θ,loss_n_ode(θ)...)\r\n\r\ndata = Iterators.repeated((), 1000)\r\nres1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05), cb = cb, maxiters=100)\r\ncb(res1.minimizer,loss_n_ode(res1.minimizer)...;doplot=true)\r\nres2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(), cb = cb)\r\ncb(res2.minimizer,loss_n_ode(res2.minimizer)...;doplot=true)","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Notice that the advantage of this format is that we can use Optim's optimizers, like LBFGS with a full Chain object for all of Flux's neural networks, like convolutional neural networks.","category":"page"},{"location":"Flux/#Using-ComponentArrays-for-neural-network-layers-1","page":"Use with Flux Chain and train!","title":"Using ComponentArrays for neural network layers","text":"","category":"section"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"We can also create the dense layers from scratch using ComponentArrays.jl. Flux is used here just for the glorot_uniform function and the ADAM optimizer.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using ComponentArrays, DiffEqFlux, Optim, OrdinaryDiffEq, Plots, UnPack\r\nusing Flux: glorot_uniform, ADAM","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Again, let's create the truth data.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"u0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0, 1.5f0)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nt = range(tspan[1], tspan[2], length = datasize)\r\nprob = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob, Tsit5(), saveat = t))","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Now, we'll make a function that creates dense neural layer components. It is similar to Flux.Dense, except it doesn't handle the activation function. We'll do that separately.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"dense_layer(in, out) = ComponentArray{Float32}(W=glorot_uniform(out, in), b=zeros(out))","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Our parameter vector will be a ComponentArray that holds the ODE initial conditions and the dense neural layers. This enables it to pass through the solver as a flat array while giving us the convenience of struct-like access to the components.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"layers = (L1=dense_layer(2, 50), L2=dense_layer(50, 2))\r\nθ = ComponentArray(u=u0, p=layers)\r\n\r\nfunction dudt(u, p, t)\r\n    @unpack L1, L2 = p\r\n    return L2.W * tanh.(L1.W * u.^3 .+ L1.b) .+ L2.b\r\nend\r\n\r\nprob = ODEProblem(dudt, u0, tspan)","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"As before, we'll define prediction and loss functions as well as a callback function to observe training.","category":"page"},{"location":"Flux/#","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"predict_n_ode(θ) = Array(solve(prob, Tsit5(), u0=θ.u, p=θ.p, saveat=t))\r\n\r\nfunction loss_n_ode(θ)\r\n    pred = predict_n_ode(θ)\r\n    loss = sum(abs2, ode_data .- pred)\r\n    return loss, pred\r\nend\r\nloss_n_ode(θ)\r\n\r\ncb = function (θ, loss, pred; doplot=false)\r\n    display(loss)\r\n    # plot current prediction against data\r\n    pl = scatter(t, ode_data[1,:], label = \"data\")\r\n    scatter!(pl, t, pred[1,:], label = \"prediction\")\r\n    display(plot(pl))\r\n    return false\r\nend\r\n\r\ncb(θ, loss_n_ode(θ)...)\r\n\r\ndata = Iterators.repeated((), 1000)\r\n\r\nres1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05); cb=cb, maxiters=100)\r\ncb(res1.minimizer, loss_n_ode(res1.minimizer)...; doplot=true)\r\n\r\nres2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(); cb=cb)\r\ncb(res2.minimizer, loss_n_ode(res2.minimizer)...; doplot=true)","category":"page"},{"location":"examples/optimal_control/#Solving-Optimal-Control-Problems-with-Universal-Differential-Equations-1","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"","category":"section"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Here we will solve a classic optimal control problem with a universal differential equation. Let","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"x^ = u^3(t)","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"where we want to optimize our controller u(t) such that the following is minimized:","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"L(theta) = sum_i Vert 4 - x(t_i) Vert + 2 Vert x^prime(t_i) Vert + Vert u(t_i) Vert","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"where i is measured on (0,8) at 0.01 intervals. To do this, we rewrite the ODE in first order form:","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"beginaligned\r\nx^prime = v \r\nv^ = u^3(t) \r\nendaligned","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"and thus","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"L(theta) = sum_i Vert 4 - x(t_i) Vert + 2 Vert v(t_i) Vert + Vert u(t_i) Vert","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"is our loss function on the first order system. We thus choose a neural network form for u and optimize the equation with respect to this loss. Note that we will first reduce control cost (the last term) by 10x in order to bump the network out of a local minimum. This looks like:","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"using DiffEqFlux, Flux, Optim, OrdinaryDiffEq, Plots, Statistics, DiffEqSensitivity\r\ntspan = (0.0f0,8.0f0)\r\nann = FastChain(FastDense(1,32,tanh), FastDense(32,32,tanh), FastDense(32,1))\r\nθ = initial_params(ann)\r\nfunction dxdt_(dx,x,p,t)\r\n    x1, x2 = x\r\n    dx[1] = x[2]\r\n    dx[2] = ann([t],p)[1]^3\r\nend\r\nx0 = [-4f0,0f0]\r\nts = Float32.(collect(0.0:0.01:tspan[2]))\r\nprob = ODEProblem(dxdt_,x0,tspan,θ)\r\nsolve(prob,Vern9(),x0,θ,abstol=1e-10,reltol=1e-10)\r\nfunction predict_adjoint(θ)\r\n  Array(solve(prob,Vern9(),p=θ,saveat=ts,sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\r\nend\r\nfunction loss_adjoint(θ)\r\n  x = predict_adjoint(θ)\r\n  mean(abs2,4.0 .- x[1,:]) + 2mean(abs2,x[2,:]) + mean(abs2,[first(ann([t],θ)) for t in ts])/10\r\nend\r\nl = loss_adjoint(θ)\r\ncb = function (θ,l)\r\n  println(l)\r\n  p = plot(solve(remake(prob,p=θ),Tsit5(),saveat=0.01),ylim=(-6,6),lw=3)\r\n  plot!(p,ts,[first(ann([t],θ)) for t in ts],label=\"u(t)\",lw=3)\r\n  display(p)\r\n  return false\r\nend\r\n# Display the ODE with the current parameter values.\r\ncb(θ,l)\r\nloss1 = loss_adjoint(θ)\r\nres1 = DiffEqFlux.sciml_train(loss_adjoint, θ, ADAM(0.005), cb = cb,maxiters=100)\r\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res1.minimizer, BFGS(initial_stepnorm=0.01), cb = cb,maxiters=100)","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Now that the system is in a better behaved part of parameter space, we return to the original loss function to finish the optimization:","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"function loss_adjoint(θ)\r\n  x = predict_adjoint(θ)\r\n  mean(abs2,4.0 .- x[1,:]) + 2mean(abs2,x[2,:]) + mean(abs2,[first(ann([t],θ)) for t in ts])\r\nend\r\n\r\nres3 = DiffEqFlux.sciml_train(loss_adjoint, res2.minimizer, BFGS(initial_stepnorm=0.01), cb = cb,maxiters=100)\r\n\r\nl = loss_adjoint(res3.minimizer)\r\ncb(res3.minimizer,l)\r\np = plot(solve(remake(prob,p=res3.minimizer),Tsit5(),saveat=0.01),ylim=(-6,6),lw=3)\r\nplot!(p,ts,[first(ann([t],res3.minimizer)) for t in ts],label=\"u(t)\",lw=3)\r\nsavefig(\"optimal_control.png\")","category":"page"},{"location":"examples/optimal_control/#","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"(Image: )","category":"page"},{"location":"Scimltrain/#sciml_train-1","page":"sciml_train","title":"sciml_train","text":"","category":"section"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"note: Note\nsciml_train is planned to be replaced by GalacticOptim.jl when it is ready. This optimizer library will have a feature superset of sciml_train but will have a slightly different interface to allow for backpropagation over the optimization and handling constrained optimization in a nicer manner. Translation from sciml_train to GalacticOptim's style will be fairly trivial since the internals are largely the same, and deprecation warnings will help you update when the time comes, so do not worry about using this functionality.","category":"page"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"sciml_train is a multi-package optimization setup. It currently allows for using the following nonlinear optimization packages as the backend:","category":"page"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"Flux.jl\nOptim.jl\nBlackBoxOptim.jl\nNLopt.jl\nMultistartOptimization.jl\nEvolutionary.jl","category":"page"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"Thus, it allows for local and global optimization, both derivative-based and derivative-free, with first and second order methods, in an easy and optimized fashion over the scientific machine learning layer functions provided by DiffEqFlux.jl. These functions come complete with integration with automatic differentiation to allow for ease of use with first and second order optimization methods, where Hessians are derived via forward-over-reverse second order sensitivity analysis.","category":"page"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"To use an optimizer from any of these libraries, one must be using the appropriate library first.","category":"page"},{"location":"Scimltrain/#API-1","page":"sciml_train","title":"API","text":"","category":"section"},{"location":"Scimltrain/#Unconstrained-Optimization-1","page":"sciml_train","title":"Unconstrained Optimization","text":"","category":"section"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"function sciml_train(loss, _θ, opt, _data = DEFAULT_DATA;\r\n                     cb = (args...) -> false,\r\n                     maxiters = get_maxiters(data),\r\n                     progress=true, save_best=true)","category":"page"},{"location":"Scimltrain/#Box-Constrained-Optimization-1","page":"sciml_train","title":"Box Constrained Optimization","text":"","category":"section"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"function sciml_train(loss, θ, opt,\r\n                     data = DEFAULT_DATA;\r\n                     lower_bounds, upper_bounds,\r\n                     cb = (args...) -> (false), maxiters = get_maxiters(data))","category":"page"},{"location":"Scimltrain/#Loss-Functions-and-Callbacks-1","page":"sciml_train","title":"Loss Functions and Callbacks","text":"","category":"section"},{"location":"Scimltrain/#","page":"sciml_train","title":"sciml_train","text":"Loss functions in sciml_train treat the first returned value as the return. For example, if one returns (1.0,[2.0]), then the value the optimizer will see is 1.0. The other values are passed to the callback function. The callback function is cb(p,args...) where the arguments are the extra returns from the loss. This allows for reusing instead of recalculating. The callback function must return a boolean where if true, then the optimizer will prematurely end the optimization. It is called after every successful step, something that is defined in an optimizer-dependent manner.","category":"page"},{"location":"examples/tensor_layer/#Physics-Informed-Machine-Learning-with-TensorLayer-1","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"","category":"section"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"In this tutorial, we show how to use the DiffEqFlux TensorLayer to solve problems in Physics Informed Machine Learning.","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Let's consider the anharmonic oscillator described by the ODE","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"ẍ = - kx - αx³ - βẋ -γẋ³.","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"To obtain the training data, we solve the equation of motion using one of the solvers in OrdinaryDiffEq:","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"using DiffEqFlux, Flux, Optim, OrdinaryDiffEq, LinearAlgebra\nk, α, β, γ = 1, 0.1, 0.2, 0.3\ntspan = (0.0,10.0)\n\nfunction dxdt_train(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\nend\n\nu0 = [1.0,0.0]\nts = collect(0.0:0.1:tspan[2])\nprob_train = ODEProblem{true}(dxdt_train,u0,tspan,p=nothing)\ndata_train = Array(solve(prob_train,Tsit5(),saveat=ts))","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Now, we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis:","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"A = [LegendreBasis(10), LegendreBasis(10)]\nnn = TensorLayer(A, 1)","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"and we also instantiate the model we are trying to learn, \"informing\" the neural about the ∝x and ∝v dependencies in the equation of motion:","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"f = x -> min(30one(x),x)\n\nfunction dxdt_pred(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]*u[1] - p[2]*u[2] + f(nn(u,p[3:end])[1])\nend\n\nα = zeros(102)\n\nprob_pred = ODEProblem{true}(dxdt_pred,u0,tspan,p=nothing)","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Note that we introduced a \"cap\" in the neural network term to avoid instabilities in the solution of the ODE. We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example.","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Finally, we introduce the corresponding loss function:","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"\nfunction predict_adjoint(θ)\n  x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts))\nend\n\nfunction loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  loss = sum(norm.(x - data_train))\n  return loss\nend\n\nfunction cb(θ,l)\n  @show θ, l\n  return false\nend","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"and we train the network using two rounds of ADAM:","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"res1 = DiffEqFlux.sciml_train(loss_adjoint, α, ADAM(0.05), cb = cb, maxiters = 150)\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res1.minimizer, ADAM(0.001), cb = cb,maxiters = 150)\nopt = res2.minimizer","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"We plot the results and we obtain a fairly accurate learned model:","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"using Plots\ndata_pred = predict_adjoint(opt)\nplot(ts, data_train[1,:], label = \"X (ODE)\")\nplot!(ts, data_train[2,:], label = \"V (ODE)\")\nplot!(ts, data_pred[1,:], label = \"X (NN)\")\nplot!(ts, data_pred[2,:],label = \"V (NN)\")","category":"page"},{"location":"examples/tensor_layer/#","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"(Image: plot_tutorial)","category":"page"},{"location":"examples/normalizing_flows/#Continuous-Normalizing-Flows-with-sciml_train-1","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"","category":"section"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"Now, we study a single layer neural network that can estimate the density p_x of a variable of interest x by re-parameterizing a base variable z with known density p_z through the Neural Network model passed to the layer.","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"We can use DiffEqFlux.jl to define, train and output the densities computed by CNF layers. In the same way as a neural ODE, the layer takes a neural network that defines its derivative function (see [1] for a reference). A possible way to define a CNF layer, would be","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Distributions, Zygote\n\nnn = Chain(Dense(1, 3, tanh), Dense(3, 1, tanh))\ntspan = (0.0,10.0)\nffjord_test = FFJORD(nn,tspan, Tsit5())","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"where we also pass as an input the desired timespan for which the differential equation that defines log p_x and z(t) will be solved.","category":"page"},{"location":"examples/normalizing_flows/#Training-a-CNF-layer-1","page":"Continuous Normalizing Flows with sciml_train","title":"Training a CNF layer","text":"","category":"section"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"First, let's get an array from a normal distribution as the training data","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"data_train = [Float32(rand(Normal(6.0,0.7))) for i in 1:100]","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"Now we define a loss function that we wish to minimize","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"function loss_adjoint(θ)\n    logpx = [ffjord_test(x,θ) for x in data_train]\n    loss = -mean(logpx)\nend","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"In this example, we wish to choose the parameters of the network such that the likelihood of the re-parameterized variable is maximized. Other loss functions may be used depending on the application. Furthermore, the CNF layer gives the log of the density of the variable x, as one may guess from the code above.","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"We then train the neural network to learn the distribution of x.","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS.","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"# Train using the ADAM optimizer\nres1 = DiffEqFlux.sciml_train(loss_adjoint, ffjord_test.p,\n                                          ADAM(0.1), cb = cb,\n                                          maxiters = 100)\n\n* Status: failure (reached maximum number of iterations)\n\n* Candidate solution\n   Minimizer: [-1.88e+00, 2.44e+00, 2.01e-01,  ...]\n   Minimum:   1.240627e+00\n\n* Found with\n   Algorithm:     ADAM\n   Initial Point: [9.33e-01, 1.13e+00, 2.92e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = NaN ≰ 0.0e+00\n   |x - x'|/|x'|          = NaN ≰ 0.0e+00\n   |f(x) - f(x')|         = NaN ≰ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = NaN ≰ 0.0e+00\n   |g(x)|                 = NaN ≰ 0.0e+00\n\n* Work counters\n   Seconds run:   204  (vs limit Inf)\n   Iterations:    100\n   f(x) calls:    100\n   ∇f(x) calls:   100","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"We then complete the training using a different optimizer starting from where ADAM stopped.","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"# Retrain using the LBFGS optimizer\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res1.minimizer,\n                                        LBFGS())\n\n* Status: success\n\n* Candidate solution\n   Minimizer: [-1.06e+00, 2.24e+00, 8.77e-01,  ...]\n   Minimum:   1.157672e+00\n\n* Found with\n   Algorithm:     L-BFGS\n   Initial Point: [-1.88e+00, 2.44e+00, 2.01e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = 0.00e+00 ≰ 0.0e+00\n   |x - x'|/|x'|          = 0.00e+00 ≰ 0.0e+00\n   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n   |g(x)|                 = 4.09e-03 ≰ 1.0e-08\n\n* Work counters\n   Seconds run:   514  (vs limit Inf)\n   Iterations:    44\n   f(x) calls:    244\n   ∇f(x) calls:   244","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"References:","category":"page"},{"location":"examples/normalizing_flows/#","page":"Continuous Normalizing Flows with sciml_train","title":"Continuous Normalizing Flows with sciml_train","text":"[1] W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, D. Duvenaud. FFJORD: Free-Form Continuous Dynamic For Scalable Reversible Generative Models. arXiv preprint at arXiv1810.01367, 2018.","category":"page"},{"location":"examples/mnist_neural_ode/#GPU-based-MNIST-Neural-ODE-Classifier-1","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Training a classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"(Step-by-step description below)","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, NNlib, MLDataUtils, Printf\nusing Flux: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    imgs, labels_raw = MNIST.traindata();\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.([x_train, y_train])); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.([x_test, y_test])); batchsize = batchsize,\n                   shuffle = false)\n    )\nend\n\n# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)\n\ndown = Chain(flatten, Dense(784, 20, tanh)) |> gpu\n\nnn = Chain(Dense(20, 10, tanh),\n           Dense(10, 10, tanh),\n           Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Chain(Dense(20, 10)) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend\n\n# Build our over-all model topology\nmodel = Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncb() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\n# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, params(down, nn_ode.p, fc), train_dataloader, opt, cb = cb)","category":"page"},{"location":"examples/mnist_neural_ode/#Step-by-Step-Description-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#Load-Packages-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Load Packages","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, NNlib, MLDataUtils, Printf\nusing Flux: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets","category":"page"},{"location":"examples/mnist_neural_ode/#GPU-1","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"A good trick used here:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fallback to the CPU.","category":"page"},{"location":"examples/mnist_neural_ode/#Load-MNIST-Dataset-into-Minibatches-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    imgs, labels_raw = MNIST.traindata();\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.([x_train, y_train])); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.([x_test, y_test])); batchsize = batchsize,\n                   shuffle = false)\n    )\nend","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"and then loaded from main:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)","category":"page"},{"location":"examples/mnist_neural_ode/#Layers-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Layers","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down = Chain(flatten, Dense(784, 20, tanh)) |> gpu\n\nnn = Chain(Dense(20, 10, tanh),\n           Dense(10, 10, tanh),\n           Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Chain(Dense(20, 10)) |> gpu","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down: This layer downsamples our images into a 20 dimensional feature vector.         It takes a 28 x 28 image, flattens it, and then passes it through a fully connected         layer with tanh activation","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn: A 3 layers Deep Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn_ode: ODE solver layer","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"fc: The final fully connected layer which maps our learned feature vector to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"|> gpu: An utility function which transfers our model to GPU, if it is available","category":"page"},{"location":"examples/mnist_neural_ode/#Array-Conversion-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Array Conversion","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"When using NeuralODE, we can use the following function as a cheap conversion of DiffEqArray from the ODE solver into a Matrix that can be used in the following layer:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"For CPU: If this function does not automatically fallback to CPU when no GPU is present, we can change gpu(x) with Array(x).","category":"page"},{"location":"examples/mnist_neural_ode/#Build-Topology-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Build Topology","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Next we connect all layers together in a single chain:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Build our over-all model topology\nmodel = Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Chain(down,\n                 nn,\n                 fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"examples/mnist_neural_ode/#Prediction-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Prediction","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"examples/mnist_neural_ode/#Accuracy-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Accuracy","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(m, train_dataloader)","category":"page"},{"location":"examples/mnist_neural_ode/#Training-Parameters-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Training Parameters","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"examples/mnist_neural_ode/#Loss-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Loss","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Cross Entropy is the loss function computed here which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"examples/mnist_neural_ode/#Optimizer-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Optimizer","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"opt = ADAM(0.05)","category":"page"},{"location":"examples/mnist_neural_ode/#CallBack-1","page":"GPU-based MNIST Neural ODE Classifier","title":"CallBack","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"cb() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"examples/mnist_neural_ode/#Train-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Train","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, params( down, nn_ode.p, fc), zip( x_train, y_train ), opt, cb = cb)","category":"page"},{"location":"examples/mnist_neural_ode/#Expected-Output-1","page":"GPU-based MNIST Neural ODE Classifier","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Iter:   1 || Train Accuracy: 16.203 || Test Accuracy: 16.933\nIter:  11 || Train Accuracy: 64.406 || Test Accuracy: 64.900\nIter:  21 || Train Accuracy: 76.656 || Test Accuracy: 76.667\nIter:  31 || Train Accuracy: 81.758 || Test Accuracy: 81.683\nIter:  41 || Train Accuracy: 81.078 || Test Accuracy: 81.967\nIter:  51 || Train Accuracy: 83.953 || Test Accuracy: 84.417\nIter:  61 || Train Accuracy: 85.266 || Test Accuracy: 85.017\nIter:  71 || Train Accuracy: 85.938 || Test Accuracy: 86.400\nIter:  81 || Train Accuracy: 84.836 || Test Accuracy: 85.533\nIter:  91 || Train Accuracy: 86.148 || Test Accuracy: 86.583\nIter: 101 || Train Accuracy: 83.859 || Test Accuracy: 84.500\nIter: 111 || Train Accuracy: 86.227 || Test Accuracy: 86.617\nIter: 121 || Train Accuracy: 87.508 || Test Accuracy: 87.200\nIter: 131 || Train Accuracy: 86.227 || Test Accuracy: 85.917\nIter: 141 || Train Accuracy: 84.453 || Test Accuracy: 84.850\nIter: 151 || Train Accuracy: 86.063 || Test Accuracy: 85.650\nIter: 161 || Train Accuracy: 88.375 || Test Accuracy: 88.033\nIter: 171 || Train Accuracy: 87.398 || Test Accuracy: 87.683\nIter: 181 || Train Accuracy: 88.070 || Test Accuracy: 88.350\nIter: 191 || Train Accuracy: 86.836 || Test Accuracy: 87.150\nIter: 201 || Train Accuracy: 89.266 || Test Accuracy: 88.583\nIter: 211 || Train Accuracy: 86.633 || Test Accuracy: 85.550\nIter: 221 || Train Accuracy: 89.313 || Test Accuracy: 88.217\nIter: 231 || Train Accuracy: 88.641 || Test Accuracy: 89.417\nIter: 241 || Train Accuracy: 88.617 || Test Accuracy: 88.550\nIter: 251 || Train Accuracy: 88.211 || Test Accuracy: 87.950\nIter: 261 || Train Accuracy: 87.742 || Test Accuracy: 87.317\nIter: 271 || Train Accuracy: 89.070 || Test Accuracy: 89.217\nIter: 281 || Train Accuracy: 89.703 || Test Accuracy: 89.067\nIter: 291 || Train Accuracy: 88.484 || Test Accuracy: 88.250\nIter: 301 || Train Accuracy: 87.898 || Test Accuracy: 88.367\nIter: 311 || Train Accuracy: 88.438 || Test Accuracy: 88.633\nIter: 321 || Train Accuracy: 88.664 || Test Accuracy: 88.567\nIter: 331 || Train Accuracy: 89.906 || Test Accuracy: 89.883\nIter: 341 || Train Accuracy: 88.883 || Test Accuracy: 88.667\nIter: 351 || Train Accuracy: 89.609 || Test Accuracy: 89.283\nIter: 361 || Train Accuracy: 89.516 || Test Accuracy: 89.117\nIter: 371 || Train Accuracy: 89.898 || Test Accuracy: 89.633\nIter: 381 || Train Accuracy: 89.055 || Test Accuracy: 89.017\nIter: 391 || Train Accuracy: 89.445 || Test Accuracy: 89.467\nIter: 401 || Train Accuracy: 89.156 || Test Accuracy: 88.250\nIter: 411 || Train Accuracy: 88.977 || Test Accuracy: 89.083\nIter: 421 || Train Accuracy: 90.109 || Test Accuracy: 89.417","category":"page"},{"location":"FastChain/#FastChain-1","page":"FastChain","title":"FastChain","text":"","category":"section"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"The FastChain system is a Flux-like explicit parameter neural network architecture system for less overhead in smaller neural networks. For neural networks with layers of lengths >~200, these optimizations are overshadowed by the cost of matrix multiplication. However, for smaller layer operations, this architecture can reduce a lot of the overhead traditionally seen in neural network architectures and thus is recommended in a lot of scientific machine learning use cases.","category":"page"},{"location":"FastChain/#Basics-1","page":"FastChain","title":"Basics","text":"","category":"section"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"The basic principle is that FastChain is a collection of functions of two values, (x,p), and chains these functions to call one after the next. Each layer in this chain gets a pre-defined amount of parameters sent to it. For example,","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"f = FastChain((x,p) -> x.^3,\r\n              FastDense(2,50,tanh),\r\n              FastDense(50,2))","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"FastChain here has a 2*50 + 50 length parameter FastDense(2,50,tanh) function and a 50*2 + 2 parameter function FastDense(50,2). The first function gets the default number of parameters which is 0. Thus, f(x,p) is equivalent to the following code:","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"function f(x,p)\r\n  tmp1 = x.^3\r\n  len1 = paramlength(FastDense(2,50,tanh))\r\n  tmp2 = FastDense(2,50,tanh)(tmp1,@view p[1:len1])\r\n  tmp3 = FastDense(50,2)(tmp2,@view p[len2:end])\r\nend","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"FastChain functions thus require that the vector of neural network parameters is passed to it on each call, making the setup explicit in the passed parameters.","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"To get initial parameters for the optimization of a function defined by a FastChain, one simply calls initial_params(f), which returns the concatenation of the initial parameters for each layer. Notice that since all parameters are explicit, constructing and reconstructing chains/layers can be a memory-free operation, since the only memory is the parameter vector itself, which is handled by the user.","category":"page"},{"location":"FastChain/#FastChain-Interface-1","page":"FastChain","title":"FastChain Interface","text":"","category":"section"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"The only requirement to be a layer in FastChain is to be a 2-argument function l(x,p) and define the following traits:","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"paramlength(::typeof(l)): The number of parameters from the parameter vector to allocate to this layer. Defaults to zero.\ninitial_params(::typeof(l)): The function for defining the initial parameters of the layer. Should output a vector of length matching paramlength. Defaults to Float32[].","category":"page"},{"location":"FastChain/#FastChain-Compatible-Layers-1","page":"FastChain","title":"FastChain-Compatible Layers","text":"","category":"section"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"The following pre-defined layers can be used with FastChain:","category":"page"},{"location":"FastChain/#","page":"FastChain","title":"FastChain","text":"FastDense\r\nStaticDense","category":"page"},{"location":"FastChain/#DiffEqFlux.FastDense","page":"FastChain","title":"DiffEqFlux.FastDense","text":"FastDense(in,out,activation=identity;           initW = Flux.glorot_uniform, initb = Flux.zeros)\n\nA Dense layer activation.(W*x + b) with input size in and output size out. The activation function defaults to identity, meaning the layer is an affine function. Initial parameters are taken to match Flux.Dense.\n\nNote that this function has specializations on tanh for a slightly faster adjoint with Zygote.\n\n\n\n\n\n","category":"type"},{"location":"FastChain/#DiffEqFlux.StaticDense","page":"FastChain","title":"DiffEqFlux.StaticDense","text":"StaticDense(in,out,activation=identity;           initW = Flux.glorot_uniform, initb = Flux.zeros)\n\nA Dense layer activation.(W*x + b) with input size in and output size out. The activation function defaults to identity, meaning the layer is an affine function. Initial parameters are taken to match Flux.Dense. The internal calculations are done with StaticArrays for extra speed for small linear algebra operations. Should only be used for input/output sizes of approximately 16 or less.\n\nNote that this function has specializations on tanh for a slightly faster adjoint with Zygote.\n\n\n\n\n\n","category":"type"},{"location":"examples/neural_sde/#Neural-Stochastic-Differential-Equations-1","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"","category":"section"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"With neural stochastic differential equations, there is once again a helper form neural_dmsde which can be used for the multiplicative noise case (consult the layers API documentation, or this full example using the layer function).","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"However, since there are far too many possible combinations for the API to support, in many cases you will want to performantly define neural differential equations for non-ODE systems from scratch. For these systems, it is generally best to use TrackerAdjoint with non-mutating (out-of-place) forms. For example, the following defines a neural SDE with neural networks for both the drift and diffusion terms:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"dudt(u, p, t) = model(u)\ng(u, p, t) = model2(u)\nprob = SDEProblem(dudt, g, x, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"where model and model2 are different neural networks. The same can apply to a neural delay differential equation. Its out-of-place formulation is f(u,h,p,t). Thus for example, if we want to define a neural delay differential equation which uses the history value at p.tau in the past, we can define:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"dudt!(u, h, p, t) = model([u; h(t - p.tau)])\nprob = DDEProblem(dudt_, u0, h, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"First let's build training data from the same example as the neural ODE:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"using Plots, Statistics\nusing Flux, DiffEqFlux, StochasticDiffEq, DiffEqBase.EnsembleAnalysis\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0, 1.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"function trueSDEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nmp = Float32[0.2, 0.2]\nfunction true_noise_func(du, u, p, t)\n    du .= mp.*u\nend\n\nprob_truesde = SDEProblem(trueSDEfunc, true_noise_func, u0, tspan)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"For our dataset we will use DifferentialEquations.jl's parallel ensemble interface to generate data from the average of 10,000 runs of the SDE:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"# Take a typical sample from the mean\nensemble_prob = EnsembleProblem(prob_truesde)\nensemble_sol = solve(ensemble_prob, SOSRI(), trajectories = 10000)\nensemble_sum = EnsembleSummary(ensemble_sol)\n\nsde_data, sde_data_vars = Array.(timeseries_point_meanvar(ensemble_sol, tsteps))","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Now we build a neural SDE. For simplicity we will use the NeuralDSDE neural SDE with diagonal noise layer function:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"drift_dudt = FastChain((x, p) -> x.^3,\n                       FastDense(2, 50, tanh),\n                       FastDense(50, 2))\ndiffusion_dudt = FastChain(FastDense(2, 2))\n\nneuralsde = NeuralDSDE(drift_dudt, diffusion_dudt, tspan, SOSRI(),\n                       saveat = tsteps, reltol = 1e-1, abstol = 1e-1)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Let's see what that looks like:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"# Get the prediction using the correct initial condition\nprediction0 = neuralsde(u0)\n\ndrift_(u, p, t) = drift_dudt(u, p[1:neuralsde.len])\ndiffusion_(u, p, t) = diffusion_dudt(u, p[(neuralsde.len+1):end])\n\nprob_neuralsde = SDEProblem(drift_, diffusion_, u0,(0.0f0, 1.2f0), neuralsde.p)\n\nensemble_nprob = EnsembleProblem(prob_neuralsde)\nensemble_nsol = solve(ensemble_nprob, SOSRI(), trajectories = 100,\n                      saveat = tsteps)\nensemble_nsum = EnsembleSummary(ensemble_nsol)\n\nplt1 = plot(ensemble_nsum, title = \"Neural SDE: Before Training\")\nscatter!(plt1, tsteps, sde_data', lw = 3)\n\nscatter(tsteps, sde_data[1,:], label = \"data\")\nscatter!(tsteps, prediction0[1,:], label = \"prediction\")","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Now just as with the neural ODE we define a loss function that calculates the mean and variance from n runs at each time point and uses the distance from the data values:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"function predict_neuralsde(p)\n  return Array(neuralsde(u0, p))\nend\n\nfunction loss_neuralsde(p; n = 100)\n  samples = [predict_neuralsde(p) for i in 1:n]\n  means = reshape(mean.([[samples[i][j] for i in 1:length(samples)]\n                                        for j in 1:length(samples[1])]),\n                      size(samples[1])...)\n  vars = reshape(var.([[samples[i][j] for i in 1:length(samples)]\n                                      for j in 1:length(samples[1])]),\n                      size(samples[1])...)\n  loss = sum(abs2, sde_data - means) + sum(abs2, sde_data_vars - vars)\n  return loss, means, vars\nend","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"list_plots = []\niter = 0\n\n# Callback function to observe training\ncallback = function (p, loss, means, vars; doplot = false)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  # loss against current data\n  display(loss)\n\n  # plot current prediction against data\n  plt = scatter(tsteps, sde_data[1,:], yerror = sde_data_vars[1,:],\n                ylim = (-4.0, 8.0), label = \"data\")\n  scatter!(plt, tsteps, means[1,:], ribbon = vars[1,:], label = \"prediction\")\n  push!(list_plots, plt)\n\n  if doplot\n    display(plt)\n  end\n  return false\nend","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Now we train using this loss function. We can pre-train a little bit using a smaller n and then decrease it after it has had some time to adjust towards the right mean behavior:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"opt = ADAM(0.025)\n\n# First round of training with n = 10\nresult1 = DiffEqFlux.sciml_train((p) -> loss_neuralsde(p, n = 10),  \n                                 neuralsde.p, opt,\n                                 cb = callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"We resume the training with a larger n. (WARNING - this step is a couple of orders of magnitude longer than the previous one).","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"result2 = DiffEqFlux.sciml_train((p) -> loss_neuralsde(p, n = 100),\n                                 result1.minimizer, opt,\n                                 cb = callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"And now we plot the solution to an ensemble of the trained neural SDE:","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"samples = [predict_neuralsde(result2.minimizer) for i in 1:1000]\nmeans = reshape(mean.([[samples[i][j] for i in 1:length(samples)]\n                                      for j in 1:length(samples[1])]),\n                    size(samples[1])...)\nvars = reshape(var.([[samples[i][j] for i in 1:length(samples)]\n                                    for j in 1:length(samples[1])]),\n                    size(samples[1])...)\n\nplt2 = scatter(tsteps, sde_data', yerror = sde_data_vars',\n               label = \"data\", title = \"Neural SDE: After Training\",\n               xlabel = \"Time\")\nplot!(plt2, tsteps, means', lw = 8, ribbon = vars', label = \"prediction\")\n\nplt = plot(plt1, plt2, layout = (2, 1))\nsavefig(plt, \"NN_sde_combined.png\"); nothing # sde","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"examples/neural_sde/#","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Try this with GPUs as well!","category":"page"},{"location":"examples/feedback_control/#Universal-Differential-Equations-for-Neural-Feedback-Control-1","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"","category":"section"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"You can also mix a known differential equation and a neural differential equation, so that the parameters and the neural network are estimated simultaneously!","category":"page"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"We will assume that we know the dynamics of the second equation (linear dynamics), and our goal is to find a neural network that is dependent on the current state of the dynamical system that will control the second equation to stay close to 1.","category":"page"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"using DiffEqFlux, Flux, Optim, OrdinaryDiffEq, Plots\n\nu0 = 1.1f0\ntspan = (0.0f0, 25.0f0)\ntsteps = 0.0f0:1.0:25.0f0\n\nmodel_univ = FastChain(FastDense(2, 16, tanh),\n                       FastDense(16, 16, tanh),\n                       FastDense(16, 1))\n\n# The model weights are destructured into a vector of parameters\np_model = initial_params(model_univ)\nn_weights = length(p_model)\n\n# Parameters of the second equation (linear dynamics)\np_system = Float32[0.5, -0.5]\n\np_all = [p_model; p_system]\nθ = Float32[u0; p_all]\n\nfunction dudt_univ!(du, u, p, t)\n    # Destructure the parameters\n    model_weights = p[1:n_weights]\n    α = p[end - 1]\n    β = p[end]\n\n    # The neural network outputs a control taken by the system\n    # The system then produces an output\n    model_control, system_output = u\n\n    # Dynamics of the control and system\n    dmodel_control = model_univ(u, model_weights)[1]\n    dsystem_output = α*system_output + β*model_control\n\n    # Update in place\n    du[1] = dmodel_control\n    du[2] = dsystem_output\nend\n\nprob_univ = ODEProblem(dudt_univ!, [0f0, u0], tspan, p_all)\nsol_univ = solve(prob_univ, Tsit5(),abstol = 1e-8, reltol = 1e-6)\n\nfunction predict_univ(θ)\n  return Array(solve(prob_univ, Tsit5(), u0=[0f0, θ[1]], p=θ[2:end],\n                              saveat = tsteps))\nend\n\nloss_univ(θ) = sum(abs2, predict_univ(θ)[2,:] .- 1)\nl = loss_univ(θ)","category":"page"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"list_plots = []\niter = 0\ncallback = function (θ, l)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  println(l)\n\n  plt = plot(predict_univ(θ)', ylim = (0, 6))\n  push!(list_plots, plt)\n  display(plt)\n  return false\nend","category":"page"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"result_univ = DiffEqFlux.sciml_train(loss_univ, θ,\n                                     BFGS(initial_stepnorm = 0.01),\n                                     cb = callback)","category":"page"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"* Status: success\n\n* Candidate solution\n   Minimizer: [1.00e+00, 4.33e-02, 3.72e-01,  ...]\n   Minimum:   6.572520e-13\n\n* Found with\n   Algorithm:     BFGS\n   Initial Point: [1.10e+00, 4.18e-02, 3.64e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = 0.00e+00 ≤ 0.0e+00\n   |x - x'|/|x'|          = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n   |g(x)|                 = 5.45e-06 ≰ 1.0e-08\n\n* Work counters\n   Seconds run:   8  (vs limit Inf)\n   Iterations:    23\n   f(x) calls:    172\n   ∇f(x) calls:   172","category":"page"},{"location":"examples/feedback_control/#","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"Notice that in just 23 iterations or 8 seconds we get to a minimum of 7e-13, successfully solving the nonlinear optimal control problem.","category":"page"},{"location":"controlling_AD/#Controlling-Automatic-Differentiation-1","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"","category":"section"},{"location":"controlling_AD/#","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"One of the key features of DiffEqFlux.jl is the fact that it has many modes of differentiation which are available, allowing neural differential equations and universal differential equations to be fit in the manner that is most appropriate.","category":"page"},{"location":"controlling_AD/#","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"To use the automatic differentiation overloads, the differential equation just needs to be solved with solve. Thus, for example,","category":"page"},{"location":"controlling_AD/#","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"using DiffEqSensitivity, OrdinaryDiffEq, Zygote\r\n\r\nfunction fiip(du,u,p,t)\r\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\r\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\r\nend\r\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\r\nprob = ODEProblem(fiip,u0,(0.0,10.0),p)\r\nsol = solve(prob,Tsit5())\r\nloss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\r\ndu0,dp = Zygote.gradient(loss,u0,p)","category":"page"},{"location":"controlling_AD/#","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"will compute the gradient of the loss function \"sum of the values of the solution to the ODE at timepoints dt=0.1\" using an adjoint method, where du0 is the derivative of the loss function with respect to the initial condition and dp is the derivative of the loss function with respect to the parameters.","category":"page"},{"location":"controlling_AD/#Choosing-a-Differentiation-Method-1","page":"Controlling Automatic Differentiation","title":"Choosing a Differentiation Method","text":"","category":"section"},{"location":"controlling_AD/#","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"The choice of the method for calculating the gradient is made by passing the keyword argument sensealg to solve. The default choice is dependent on the type of differential equation and the choice of neural network architecture.","category":"page"},{"location":"controlling_AD/#","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"The full listing of differentiation methods is described in the DifferentialEquations.jl documentation. That page also has guidelines on how to make the right choice.","category":"page"},{"location":"examples/delay_diffeq/#Delay-Differential-Equations-1","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"","category":"section"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Other differential equation problem types from DifferentialEquations.jl are supported. For example, we can build a layer with a delay differential equation like:","category":"page"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"using DifferentialEquations, Flux, Optim, DiffEqFlux, DiffEqSensitivity\n\n\n# Define the same LV equation, but including a delay parameter\nfunction delay_lotka_volterra!(du, u, h, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = (α   - β*y) * h(p, t-0.1)[1]\n  du[2] = dy = (δ*x - γ)   * y\nend\n\n# Initial parameters\np = [2.2, 1.0, 2.0, 0.4]\n\n# Define a vector containing delays for each variable (although only the first\n# one is used)\nh(p, t) = ones(eltype(p), 2)\n\n# Initial conditions\nu0 = [1.0, 1.0]\n\n# Define the problem as a delay differential equation\nprob_dde = DDEProblem(delay_lotka_volterra!, u0, h, (0.0, 10.0),\n                      constant_lags = [0.1])\n\nfunction predict_dde(p)\n  return Array(solve(prob_dde, MethodOfSteps(Tsit5()),\n                              u0=u0, p=p, saveat = 0.1,\n                              sensealg = TrackerAdjoint()))\nend\n\nloss_dde(p) = sum(abs2, x-1 for x in predict_dde(p))","category":"page"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Notice that we chose sensealg = TrackerAdjoint() to utilize the Tracker.jl reverse-mode to handle the delay differential equation.","category":"page"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We define a callback to display the solution at the current parameters for each step of the training:","category":"page"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"#using Plots\ncb = function (p,l...)\n  display(loss_dde(p))\n  #display(plot(solve(remake(prob_dde,p=p),MethodOfSteps(Tsit5()),saveat=0.1),ylim=(0,6)))\n  return false\nend\n\ncb(p,loss_dde(p))","category":"page"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We use sciml_train to optimize the parameters for our loss function:","category":"page"},{"location":"examples/delay_diffeq/#","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"result_dde = DiffEqFlux.sciml_train(loss_dde, p, ADAM(0.1),\n                                    cb = cb, maxiters = 100)","category":"page"},{"location":"examples/jump/#Neural-Jump-Diffusions-(Neural-Jump-SDE)-and-Neural-Partial-Differential-Equations-(Neural-PDEs)-1","page":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","title":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","text":"","category":"section"},{"location":"examples/jump/#","page":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","title":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","text":"For the sake of not having a never-ending documentation of every single combination of CPU/GPU with every layer and every neural differential equation, we will end here. But you may want to consult this blog post which showcases defining neural jump diffusions and neural partial differential equations.","category":"page"},{"location":"examples/physical_constraints/#Enforcing-Physical-Constraints-via-Universal-Differential-Algebraic-Equations-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"As shown in the stiff ODE tutorial, differential-algebraic equations (DAEs) can be used to impose physical constraints. One way to define a DAE is through an ODE with a singular mass matrix. For example, if we make Mu' = f(u) where the last row of M is all zeros, then we have a constraint defined by the right hand side. Using NeuralODEMM, we can use this to define a neural ODE where the sum of all 3 terms must add to one. An example of this is as follows:","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"using Flux, DiffEqFlux, OrdinaryDiffEq, Optim, Test, Plots\n\n# A desired MWE for now, not a test yet.\nfunction f!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁*y₁ + k₃*y₂*y₃\n    du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n    du[3] =  y₁ + y₂ + y₃ - 1\n    return nothing\nend\n\nu₀ = [1.0, 0, 0]\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\n\ntspan = (0.0,1.0)\np = [0.04, 3e7, 1e4]\n\nstiff_func = ODEFunction(f!, mass_matrix = M)\nprob_stiff = ODEProblem(stiff_func, u₀, tspan, p)\nsol_stiff = solve(prob_stiff, Rodas5(), saveat = 0.1)\n\nnn_dudt2 = FastChain(FastDense(3, 64, tanh),\n                     FastDense(64, 2))\n\nmodel_stiff_ndae = NeuralODEMM(nn_dudt2, (u, p, t) -> [u[1] + u[2] + u[3] - 1],\n                               tspan, M, Rodas5(), saveat = 0.1)\nmodel_stiff_ndae(u₀)\n\nfunction predict_stiff_ndae(p)\n    return model_stiff_ndae(u₀, p)\nend\n\nfunction loss_stiff_ndae(p)\n    pred = predict_stiff_ndae(p)\n    loss = sum(abs2, Array(sol_stiff) .- pred)\n    return loss, pred\nend\n\ncallback = function (p, l, pred) #callback function to observe training\n  display(l)\n  return false\nend\n\nl1 = first(loss_stiff_ndae(model_stiff_ndae.p))\nresult_stiff = DiffEqFlux.sciml_train(loss_stiff_ndae, model_stiff_ndae.p,\n                                      BFGS(initial_stepnorm = 0.001),\n                                      cb = callback, maxiters = 100)","category":"page"},{"location":"examples/physical_constraints/#Step-by-Step-Description-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/physical_constraints/#Load-Packages-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Load Packages","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"using Flux, DiffEqFlux, OrdinaryDiffEq, Optim, Test, Plots","category":"page"},{"location":"examples/physical_constraints/#Differential-Equation-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Differential Equation","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"First, we define our differential equations as a highly stiff problem which makes the fitting difficult.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"# A desired MWE for now, not a test yet.\nfunction f!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁*y₁ + k₃*y₂*y₃\n    du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n    du[3] =  y₁ + y₂ + y₃ - 1\n    return nothing\nend","category":"page"},{"location":"examples/physical_constraints/#Parameters-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Parameters","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"u₀ = [1.0, 0, 0]\n\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\n\ntspan = (0.0,1.0)\n\np = [0.04, 3e7, 1e4]","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"u₀ = Initial Conditions\nM = Semi-explicit Mass Matrix (last row is the constraint equation and are therefore","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"all zeros)","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"tspan = Time span over which to evaluate\np = parameters k1, k2 and k3 of the differential equation above","category":"page"},{"location":"examples/physical_constraints/#ODE-Function,-Problem-and-Solution-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"ODE Function, Problem and Solution","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"We define and solve our ODE problem to generate the \"labeled\" data which will be used to train our Neural Network.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"stiff_func = ODEFunction(f!, mass_matrix = M)\nprob_stiff = ODEProblem(stiff_func, u₀, tspan, p)\nsol_stiff = solve(prob_stiff, Rodas5(), saveat = 0.1)","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Because this is a DAE we need to make sure to use a compatible solver. Rodas5 works well for this example.","category":"page"},{"location":"examples/physical_constraints/#Neural-Network-Layers-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Neural Network Layers","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Next, we create our layers using FastChain. We use this instead of Chain because it reduces the overhead making it faster for smaller NNs of <200 layers (similarly for FastDense). The input to our network will be the initial conditions fed in as u₀.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"nn_dudt2 = FastChain(FastDense(3, 64, tanh),\n                     FastDense(64, 2))\n\nmodel_stiff_ndae = NeuralODEMM(nn_dudt2, (u, p, t) -> [u[1] + u[2] + u[3] - 1],\n                               tspan, M, Rodas5(autodiff = false), saveat = 0.1)\nmodel_stiff_ndae(u₀)","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Because this is a stiff problem, we have manually imposed that sum constraint via (u,p,t) -> [u[1] + u[2] + u[3] - 1], making the fitting easier.","category":"page"},{"location":"examples/physical_constraints/#Prediction-Function-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Prediction Function","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"For simplicity, we define a wrapper function that only takes in the model's parameters to make predictions.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function predict_stiff_ndae(p)\n    return model_stiff_ndae(u₀, p)\nend","category":"page"},{"location":"examples/physical_constraints/#Train-Parameters-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Train Parameters","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Training our network requires a loss function, an optimizer and a callback function to display the progress.","category":"page"},{"location":"examples/physical_constraints/#Loss-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Loss","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"We first make our predictions based on the current parameters, then calculate the loss from these predictions. In this case, we use least squares as our loss.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function loss_stiff_ndae(p)\n    pred = predict_stiff_ndae(p)\n    loss = sum(abs2, sol_stiff .- pred)\n    return loss, pred\nend\n\nl1 = first(loss_stiff_ndae(model_stiff_ndae.p))","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Notice that we are feeding the parameters of model_stiff_ndae to the loss_stiff_ndae function. model_stiff_node.p are the weights of our NN and is of size 386 (4 * 64 + 65 * 2) including the biases.","category":"page"},{"location":"examples/physical_constraints/#Optimizer-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"The optimizer BFGS is directly passed in the training step (see below).","category":"page"},{"location":"examples/physical_constraints/#Callback-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Callback","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"The callback function displays the loss during training.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"callback = function (p, l, pred) #callback function to observe training\n  display(l)\n  return false\nend","category":"page"},{"location":"examples/physical_constraints/#Train-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Train","text":"","category":"section"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Finally, training with sciml_train by passing: loss function, model parameters, optimizer, callback and maximum iteration.","category":"page"},{"location":"examples/physical_constraints/#","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"result_stiff = DiffEqFlux.sciml_train(loss_stiff_ndae, model_stiff_ndae.p,\n                                      BFGS(initial_stepnorm = 0.001),\n                                      cb = callback, maxiters = 100)","category":"page"},{"location":"examples/physical_constraints/#Expected-Output-1","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/minibatch/#Training-a-Neural-Ordinary-Differntial-Equation-with-Mini-Batching-1","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"","category":"section"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"When training universal differential equations it is often helpful to batch our data. This is particularly useful when working with large sets of training data. Let us take a look at how this works with the Lotka-Volterra equation. ","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"We first get a time series array from the Lotka-Volterra equation as data:","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"using DifferentialEquations, Flux, Optim, DiffEqFlux\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0,1.5f0)\n\nfunction trueODEfunc(du,u,p,t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nt = range(tspan[1],tspan[2],length=datasize)\nprob = ODEProblem(trueODEfunc,u0,tspan)\node_data = Array(solve(prob,Tsit5(),saveat=t))","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"Now let's define a neural network with a NeuralODE layer. First we define the layer. Here we're going to use FastChain, which is a faster neural network structure for NeuralODEs:","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"dudt2 = FastChain((x,p) -> x.^3,\n            FastDense(2,50,tanh),\n            FastDense(50,2))\nn_ode = NeuralODE(dudt2,tspan,Tsit5(),saveat=t)","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"In our model we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop. We will use the network's output against the time series data. To add support for batches of size k we need to add parameters representing the start index and size of our batch to our loss function:","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"function predict_n_ode(p)\n    n_ode(u0,p)\n  end\n  \n  function loss_n_ode(p, start, k)\n      pred = predict_n_ode(p)\n      loss = sum(abs2,ode_data[:,start:start+k] .- pred[:,start:start+k])\n      loss,pred\n  end\n  ","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"and then create a generator, that produces MAX_BATCHES random tuples (start index, batch size):","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"  MAX_BATCHES = 1000\n  k = 15 #batch size\n  data = ((rand(1:size(ode_data)[2] -k), k) for i in 1:MAX_BATCHES)","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"and then train the neural network to learn the ODE:","category":"page"},{"location":"examples/minibatch/#","page":"Training a Neural Ordinary Differntial Equation with Mini-Batching","title":"Training a Neural Ordinary Differntial Equation with Mini-Batching","text":"\n  cb = function (p,l,pred;doplot=false) #callback function to observe training\n    display(l)\n    # plot current prediction against data\n    if doplot\n      pl = scatter(t,ode_data[1,:],label=\"data\")\n      scatter!(pl,t,pred[1,:],label=\"prediction\")\n      display(plot(pl))\n    end\n    return false\n  end\n\n\n \nres = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(), data, cb = cb)\n","category":"page"},{"location":"examples/optimization_ode/#Optimization-of-Ordinary-Differential-Equations-1","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"First let's create a Lotka-Volterra ODE using DifferentialEquations.jl. For more details, see the DifferentialEquations.jl documentation. The Lotka-Volterra equations have the form:","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"beginaligned\nfracdxdt = alpha x - beta x y      \nfracdydt = -delta y + gamma x y    \nendaligned","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"using DifferentialEquations, Flux, Optim, DiffEqFlux, DiffEqSensitivity, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\n# LV equation parameter. p = [α, β, δ, γ]\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setup the ODE problem, then solve\nprob_ode = ODEProblem(lotka_volterra!, u0, tspan, p)\nsol_ode = solve(prob_ode, Tsit5())\n\n# Plot the solution\nusing Plots\nplot(sol_ode)\nsavefig(\"LV_ode.png\")","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"(Image: LV Solution Plot)","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"For this first example, we do not yet include a neural network. We take AD-compatible solve function function that takes the parameters and an initial condition and returns the solution of the differential equation as a DiffEqArray (same array semantics as the standard differential equation solution object but without the interpolations).","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"# Create a solution (prediction) for a given starting point u0 and set of\n# parameters p\nfunction predict_adjoint(p)\n  return Array(solve(prob_ode, Tsit5(), p=p, saveat = tsteps))\nend","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Next we choose a square loss function. Our goal will be to find parameter that make the Lotka-Volterra solution constant x(t)=1, so we defined our loss as the squared distance from 1. Note that when using sciml_train, the first return is the loss value, and the other returns are sent to the callback for monitoring convergence.","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"function loss_adjoint(p)\n  prediction = predict_adjoint(p)\n  loss = sum(abs2, x-1 for x in prediction)\n  return loss, prediction\nend","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Lastly, we use the sciml_train function to train the parameters using BFGS to arrive at parameters which optimize for our goal. sciml_train allows defining a callback that will be called at each step of our training loop. It takes in the current parameter vector and the returns of the last call to the loss function. We will display the current loss and make a plot of the current situation:","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred)\n  global iter, list_plots\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  display(l)\n\n  # using `remake` to re-create our `prob` with current parameters `p`\n  remade_solution = solve(remake(prob_ode, p = p), Tsit5(), saveat = tsteps)\n  plt = plot(remade_solution, ylim = (0, 6))\n\n  push!(list_plots, plt)\n  display(plt)\n\n  # Tell sciml_train to not halt the optimization. If return true, then\n  # optimization stops.\n  return false\nend","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Let's optimise the model.","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"result_ode = DiffEqFlux.sciml_train(loss_adjoint, p,\n                                    BFGS(initial_stepnorm = 0.0001),\n                                    cb = callback)","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"In just seconds we found parameters which give a relative loss of 1e-6! We can get the final loss with result_ode.minimum, and get the optimal parameters with result_ode.minimizer. For example, we can plot the final outcome and show that we solved the control problem and successfully found parameters to make the ODE solution constant:","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"remade_solution = solve(remake(prob_ode, p = result_ode.minimizer), Tsit5(),      \n                        saveat = tsteps)\n#plot(remade_solution, ylim = (0, 6))\n#savefig(\"LV_ode2.png\")","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"(Image: Final plot)","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"This shows the evolution of the solutions:","category":"page"},{"location":"examples/optimization_ode/#","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"animate(list_plots) # hide","category":"page"},{"location":"examples/universal_diffeq/#Universal-Ordinary,-Stochastic,-and-Partial-Diffrential-Equation-Examples-1","page":"Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples","title":"Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples","text":"","category":"section"},{"location":"examples/universal_diffeq/#","page":"Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples","title":"Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples","text":"For examples of using universal ordinary and stochastic differential equations, along with universal partial differential equations, see the Universal Differential Equations for Scientific Machine Learning paper along with its examples repository","category":"page"},{"location":"basis_docs/#Basis-1","page":"Basis","title":"Basis","text":"","category":"section"},{"location":"basis_docs/#","page":"Basis","title":"Basis","text":"The following basis are helper functions for easily building arrays of the form [f0(x), ..., f{n-1}(x)], where f is the corresponding function of the basis (e.g, Chebyshev Polynomials, Legendre Polynomials, etc.)","category":"page"},{"location":"basis_docs/#","page":"Basis","title":"Basis","text":"ChebyshevBasis\nSinBasis\nCosBasis\nFourierBasis\nLegendreBasis\nPolynomialBasis","category":"page"},{"location":"basis_docs/#DiffEqFlux.ChebyshevBasis","page":"Basis","title":"DiffEqFlux.ChebyshevBasis","text":"Constructs a Chebyshev basis of the form [T{0}(x), T{1}(x), ..., T{n-1}(x)] where Tj(.) is the j-th Chebyshev polynomial of the first kind.\n\nChebyshevBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"basis_docs/#DiffEqFlux.SinBasis","page":"Basis","title":"DiffEqFlux.SinBasis","text":"Constructs a sine basis of the form [sin(x), sin(2x), ..., sin(nx)].\n\nSinBasis(n)\n\nArguments:\n\nn: number of terms in the sine expansion.\n\n\n\n\n\n","category":"type"},{"location":"basis_docs/#DiffEqFlux.CosBasis","page":"Basis","title":"DiffEqFlux.CosBasis","text":"Constructs a cosine basis of the form [cos(x), cos(2x), ..., cos(nx)].\n\nCosBasis(n)\n\nArguments:\n\nn: number of terms in the cosine expansion.\n\n\n\n\n\n","category":"type"},{"location":"basis_docs/#DiffEqFlux.FourierBasis","page":"Basis","title":"DiffEqFlux.FourierBasis","text":"Constructs a Fourier basis of the form Fj(x) = j is even ? cos((j÷2)x) : sin((j÷2)x) => [F0(x), F1(x), ..., Fn(x)].\n\nFourierBasis(n)\n\nArguments:\n\nn: number of terms in the Fourier expansion.\n\n\n\n\n\n","category":"type"},{"location":"basis_docs/#DiffEqFlux.LegendreBasis","page":"Basis","title":"DiffEqFlux.LegendreBasis","text":"Constructs a Legendre basis of the form [P{0}(x), P{1}(x), ..., P{n-1}(x)] where Pj(.) is the j-th Legendre polynomial.\n\nLegendreBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"basis_docs/#DiffEqFlux.PolynomialBasis","page":"Basis","title":"DiffEqFlux.PolynomialBasis","text":"Constructs a Polynomial basis of the form [1, x, ..., x^(n-1)].\n\nPolynomialBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"examples/augmented_neural_ode/#Augmented-Neural-Ordinary-Differential-Equations-1","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using Flux, DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nimport Flux.Data: DataLoader\n\nfunction random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend\n\nfunction concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader(data |> gpu, labels |> gpu; batchsize=batch_size, shuffle=true,\n                      partial=false)\nend\n\ndiffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    return Chain((x, p=node.p) -> node(x, p),\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend\n\nfunction plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n    \n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend\n\nloss_node(x, y) = mean((model(x) .- y) .^ 2)\n\nprintln(\"Generating Dataset\")\n\ndataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)\n\ncb = function()\n    global iter += 1\n    if iter % 10 == 0\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend\n\nmodel, parameters = construct_model(1, 2, 64, 0)\nopt = ADAM(0.005)\niter = 0\n\nprintln(\"Training Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([parameters, model]), dataloader, opt, cb = cb)\nend\n\nplt_node = plot_contour(model)\n\nmodel, parameters = construct_model(1, 2, 64, 1)\nopt = ADAM(0.005)\niter = 0\n\nprintln()\nprintln(\"Training Augmented Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([parameters, model]), dataloader, opt, cb = cb)\nend\n\nplt_anode = plot_contour(model)","category":"page"},{"location":"examples/augmented_neural_ode/#Step-by-Step-Explaination-1","page":"Augmented Neural Ordinary Differential Equations","title":"Step-by-Step Explaination","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loading-required-packages-1","page":"Augmented Neural Ordinary Differential Equations","title":"Loading required packages","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using Flux, DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nimport Flux.Data: DataLoader","category":"page"},{"location":"examples/augmented_neural_ode/#Generating-a-toy-dataset-1","page":"Augmented Neural Ordinary Differential Equations","title":"Generating a toy dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign 1 to any point which lies inside the inner circle, and -1 to any point which lies between the inner and outer circle. Our first function random_point_in_sphere samples points uniformly between 2 concentric circles/spheres of radii min_radius and max_radius respectively.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader(data |> gpu, labels |> gpu; batchsize=batch_size, shuffle=true,\n                      partial=false)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Models-1","page":"Augmented Neural Ordinary Differential Equations","title":"Models","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We consider 2 models in this tuturial. The first is a simple Neural ODE which is described in detail in this tutorial. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size size(x, 1) + augment_dim instead of size(x, 1) and construct that layer accordingly.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In order to run the models on GPU, we need to manually transfer the models to GPU. First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"diffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : (AugmentedNDELayer(node, augment_dim) |> gpu)\n    return Chain((x, p=node.p) -> node(x, p),\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Plotting-the-Results-1","page":"Augmented Neural Ordinary Differential Equations","title":"Plotting the Results","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here, we define an utility to plot our model regression results as a heatmap.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n    \n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Training-Parameters-1","page":"Augmented Neural Ordinary Differential Equations","title":"Training Parameters","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loss-Functions-1","page":"Augmented Neural Ordinary Differential Equations","title":"Loss Functions","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use the L2 distance between the model prediction model(x) and the actual prediction y as the optimization objective.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"loss_node(x, y) = mean((model(x) .- y) .^ 2)","category":"page"},{"location":"examples/augmented_neural_ode/#Dataset-1","page":"Augmented Neural Ordinary Differential Equations","title":"Dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of 4000 data points.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)","category":"page"},{"location":"examples/augmented_neural_ode/#Callback-Function-1","page":"Augmented Neural Ordinary Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Additionally we define a callback function which displays the total loss at specific intervals.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"cb = function()\n    global iter += 1\n    if iter % 10 == 1\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Optimizer-1","page":"Augmented Neural Ordinary Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use ADAM as the optimizer with a learning rate of 0.005","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"opt = ADAM(0.005)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Neural-ODE-1","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"To train our neural ode model, we need to pass the appropriate learnable parameters, parameters which is returned by the construct_models function. It is simply the node.p vector. We then train our model for 20 epochs.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 0)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([model, parameters]), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: node)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Augmented-Neural-ODE-1","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Augmented Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Our training configuration will be same as that of Neural ODE. Only in this case we have augmented the input with a single zero. This makes the problem 3 dimensional and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs please refer to [1].","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 1)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([model, parameters]), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"For the augmented Neural ODE we notice that the artifact is gone.","category":"page"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: anode)","category":"page"},{"location":"examples/augmented_neural_ode/#Expected-Output-1","page":"Augmented Neural Ordinary Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Generating Dataset\nTraining Neural ODE\nIteration 10 || Loss = 0.9802582\nIteration 20 || Loss = 0.6727416\nIteration 30 || Loss = 0.5862373\nIteration 40 || Loss = 0.5278132\nIteration 50 || Loss = 0.4867624\nIteration 60 || Loss = 0.41630346\nIteration 70 || Loss = 0.3325938\nIteration 80 || Loss = 0.28235924\nIteration 90 || Loss = 0.24069068\nIteration 100 || Loss = 0.20503852\nIteration 110 || Loss = 0.17608969\nIteration 120 || Loss = 0.1491399\nIteration 130 || Loss = 0.12711425\nIteration 140 || Loss = 0.10686825\nIteration 150 || Loss = 0.089558244\n\nTraining Augmented Neural ODE\nIteration 10 || Loss = 1.3911372\nIteration 20 || Loss = 0.7694144\nIteration 30 || Loss = 0.5639633\nIteration 40 || Loss = 0.33187616\nIteration 50 || Loss = 0.14787851\nIteration 60 || Loss = 0.094676435\nIteration 70 || Loss = 0.07363529\nIteration 80 || Loss = 0.060333826\nIteration 90 || Loss = 0.04998395\nIteration 100 || Loss = 0.044843454\nIteration 110 || Loss = 0.042587914\nIteration 120 || Loss = 0.042706195\nIteration 130 || Loss = 0.040252227\nIteration 140 || Loss = 0.037686247\nIteration 150 || Loss = 0.036247417","category":"page"},{"location":"examples/augmented_neural_ode/#References-1","page":"Augmented Neural Ordinary Differential Equations","title":"References","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural odes.\" Advances in Neural Information Processing Systems. 2019.","category":"page"},{"location":"GPUs/#Use-with-GPUs-1","page":"GPUs","title":"Use with GPUs","text":"","category":"section"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array. Thus, for example, we can define a neural ODE by hand that runs on the GPU (if no GPU is available, the calculation defaults back to the CPU):","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"using DifferentialEquations, Flux, Optim, DiffEqFlux, DiffEqSensitivity\n\nmodel_gpu = Chain(Dense(2, 50, tanh), Dense(50, 2)) |> gpu\np, re = Flux.destructure(model_gpu)\ndudt!(u, p, t) = re(p)(u)\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\nu0 = Float32[2.0; 0.0] |> gpu\nprob_gpu = ODEProblem(dudt!, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"Or we could directly use the neural ODE layer function, like:","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"prob_neuralode_gpu = NeuralODE(gpu(dudt2), tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"If one is using FastChain, then the computation takes place on the GPU with f(x,p) if x and p are on the GPU. This commonly looks like:","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"dudt2 = FastChain((x,p) -> x.^3,\n            FastDense(2,50,tanh),\n            FastDense(50,2))\n\nu0 = Float32[2.; 0.] |> gpu\np = initial_params(dudt2) |> gpu\n\ndudt2_(u, p, t) = dudt2(u,p)\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\nprob_gpu = ODEProblem(dudt2_, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"or via the NeuralODE struct:","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"prob_neuralode_gpu = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nsol_gpu = solve(prob_neuralode_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/#Neural-ODE-Example-1","page":"GPUs","title":"Neural ODE Example","text":"","category":"section"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"Here is the full neural ODE example. Note that we use the gpu function so that the same code works on CPUs and GPUs, dependent on using CuArrays.","category":"page"},{"location":"GPUs/#","page":"GPUs","title":"GPUs","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, CuArrays, DiffEqSensitivity\nCuArrays.allowscalar(false) # Makes sure no slow operations are occuring\n\n# Generate Data\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n# Make the data into a GPU-based array if the user has a GPU\node_data = gpu(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nu0 = Float32[2.0; 0.0] |> gpu\np = initial_params(dudt2) |> gpu\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  gpu(prob_neuralode(u0,p))\nend\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global list_plots, iter\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, Array(ode_data[1,:]), label = \"data\")\n  scatter!(plt, tsteps, Array(pred[1,:]), label = \"prediction\")\n  push!(list_plots, plt)\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 300)","category":"page"},{"location":"examples/neural_ode_sciml/#Neural-Ordinary-Differential-Equations-with-sciml_train-1","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"","category":"section"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Next we define a single layer neural network that using the AD-compatible solve function function that takes the parameters and an initial condition and returns the solution of the differential equation as a DiffEqArray (same array semantics as the standard differential equation solution object but without the interpolations).","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"We can use DiffEqFlux.jl to define, solve, and train neural ordinary differential equations. A neural ODE is an ODE where a neural network defines its derivative function. Thus for example, with the multilayer perceptron neural network Chain(Dense(2, 50, tanh), Dense(50, 2)), the best way to define a neural ODE by hand would be to use non-mutating adjoints, which looks like:","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"model = Chain(Dense(2, 50, tanh), Dense(50, 2))\np, re = Flux.destructure(model)\ndudt!(u, p, t) = re(p)(u)\nu0 = rand(2)\nprob = ODEProblem(dudt!, u0, tspan, p)\nmy_neural_ode_prob = solve(prob, Tsit5(), args...; kwargs...)\nnothing","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"(Flux.restructure and Flux.destructure are helper functions which transform the neural network to use parameters p)","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"A convenience function which handles all of the details is NeuralODE. To use NeuralODE, you give it the initial condition, the internal neural network model to use, the timespan to solve on, and any ODE solver arguments. For example, this neural ODE would be defined as:","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"tspan = (0.0f0, 25.0f0)\nn_ode = NeuralODE(model, tspan, Tsit5(), saveat = 0.1)\nnothing","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"where here we made it a layer that takes in the initial condition and spits out an array for the time series saved at every 0.1 time steps.","category":"page"},{"location":"examples/neural_ode_sciml/#Training-a-Neural-Ordinary-Differential-Equation-1","page":"Neural Ordinary Differential Equations with sciml_train","title":"Training a Neural Ordinary Differential Equation","text":"","category":"section"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Let's get a time series array from the Lotka-Volterra equation as data:","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Now let's define a neural network with a NeuralODE layer. First we define the layer. Here we're going to use FastChain, which is a faster neural network structure for NeuralODEs:","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"dudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Note that we can directly use Chains from Flux.jl as well, for example:","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"dudt2 = Chain(x -> x.^3,\n              Dense(2, 50, tanh),\n              Dense(50, 2))","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"In our model we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop. We will use the L2 loss of the network's output against the time series data:","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"We define a callback function.","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  display(l)\n\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  push!(list_plots, plt)\n  if doplot\n    display(plot(plt))\n  end\n\n  return false\nend","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"We then train the neural network to learn the ODE.","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS. By using the two together, we are able to fit the neural ODE in 9 seconds! (Note, the timing commented out the plotting).","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"# Train using the ADAM optimizer\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 300)\n\n* Status: failure (reached maximum number of iterations)\n\n* Candidate solution\n   Minimizer: [4.38e-01, -6.02e-01, 4.98e-01,  ...]\n   Minimum:   8.691715e-02\n\n* Found with\n   Algorithm:     ADAM\n   Initial Point: [-3.02e-02, -5.40e-02, 2.78e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = NaN ≰ 0.0e+00\n   |x - x'|/|x'|          = NaN ≰ 0.0e+00\n   |f(x) - f(x')|         = NaN ≰ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = NaN ≰ 0.0e+00\n   |g(x)|                 = NaN ≰ 0.0e+00\n\n* Work counters\n   Seconds run:   5  (vs limit Inf)\n   Iterations:    300\n   f(x) calls:    300\n   ∇f(x) calls:   300","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"We then complete the training using a different optimizer starting from where ADAM stopped.","category":"page"},{"location":"examples/neural_ode_sciml/#","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"# Retrain using the LBFGS optimizer\nresult_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode,\n                                           result_neuralode.minimizer,\n                                           LBFGS(),\n                                           cb = callback)\n\n* Status: success\n\n* Candidate solution\n   Minimizer: [4.23e-01, -6.24e-01, 4.41e-01,  ...]\n   Minimum:   1.429496e-02\n\n* Found with\n   Algorithm:     L-BFGS\n   Initial Point: [4.38e-01, -6.02e-01, 4.98e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = 1.46e-11 ≰ 0.0e+00\n   |x - x'|/|x'|          = 1.26e-11 ≰ 0.0e+00\n   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n   |g(x)|                 = 4.28e-02 ≰ 1.0e-08\n\n* Work counters\n   Seconds run:   4  (vs limit Inf)\n   Iterations:    35\n   f(x) calls:    336\n   ∇f(x) calls:   336","category":"page"},{"location":"#DiffEqFlux-1","page":"Home","title":"DiffEqFlux","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"DiffEqFlux.jl is not just for neural ordinary differential equations. DiffEqFlux.jl is for universal differential equations, where these can include delays, physical constraints, stochasticity, events, and all other kinds of interesting behavior that shows up in scientific simulations. Neural networks can be all or part of the model. They can be around the differential equation, in the cost function, or inside of the differential equation. Neural networks representing unknown portions of the model or functions can go anywhere you have uncertainty in the form of the scientific simulator. For an overview of the topic with applications, consult the paper Universal Differential Equations for Scientific Machine Learning","category":"page"},{"location":"#","page":"Home","title":"Home","text":"As such, it is the first package to support and demonstrate:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Stiff universal ordinary differential equations (universal ODEs)\nUniversal stochastic differential equations (universal SDEs)\nUniversal delay differential equations (universal DDEs)\nUniversal partial differential equations (universal PDEs)\nUniversal jump stochastic differential equations (universal jump diffusions)\nHybrid universal differential equations (universal DEs with event handling)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"with high order, adaptive, implicit, GPU-accelerated, Newton-Krylov, etc. methods. For examples, please refer to the release blog post (which we try to keep updated for changes to the libraries). Additional demonstrations, like neural PDEs and neural jump SDEs, can be found at this blog post (among many others!).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Do not limit yourself to the current neuralization. With this package, you can explore various ways to integrate the two methodologies:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Neural networks can be defined where the “activations” are nonlinear functions described by differential equations\nNeural networks can be defined where some layers are ODE solves\nODEs can be defined where some terms are neural networks\nCost functions on ODEs can define neural networks","category":"page"},{"location":"#Basics-1","page":"Home","title":"Basics","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The basics are all provided by the DifferentialEquations.jl package. Specifically, the solve function is automatically compatible with AD systems like Zygote.jl and thus there is no machinery that is necessary to use DifferentialEquations.jl package. For example, the following computes the solution to an ODE and computes the gradient of a loss function (the sum of the ODE's output at each timepoint with dt=0.1) via the adjoint method:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using DiffEqSensitivity, OrdinaryDiffEq, Zygote\n\nfunction fiip(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(fiip,u0,(0.0,10.0),p)\nsol = solve(prob,Tsit5())\nloss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\ndu01,dp1 = Zygote.gradient(loss,u0,p)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Thus, what DiffEqFlux.jl provides is:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A bunch of tutorials, documentation, and test cases for this combination with neural network libraries and GPUs\nPre-built layer functions for common use cases, like neural ODEs\nSpecialized layer functions (FastDense) to improve neural differential equation training performance\nA specialized optimization function sciml_train with a training loop that allows non-machine learning libraries to be easily utilized","category":"page"},{"location":"#Citation-1","page":"Home","title":"Citation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"If you use DiffEqFlux.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"examples/second_order_neural/#Neural-Second-Order-Ordinary-Differential-Equation-1","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"","category":"section"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"The neural ODE focuses and finding a neural network such that:","category":"page"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"u^prime = NN(u)","category":"page"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"However, in many cases in physics-based modeling, the key object is not the velocity but the acceleration: knowing the acceleration tells you the force field and thus the generating process for the dynamical system. Thus what we want to do is find the force, i.e.:","category":"page"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"u^primeprime = NN(u)","category":"page"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"(Note that in order to be the acceleration, we should divide the output of the neural network by the mass!)","category":"page"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"An example of training a neural network on a second order ODE is as follows:","category":"page"},{"location":"examples/second_order_neural/#","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"using OrdinaryDiffEq, Flux, DiffEqFlux, DiffEqSensitivity, Zygote, RecursiveArrayTools\r\n\r\nu0 = Float32[0.; 2.]\r\ndu0 = Float32[0.; 0.]\r\ntspan = (0.0f0, 1.0f0)\r\nt = range(tspan[1], tspan[2], length=20)\r\n\r\nmodel = FastChain(FastDense(2, 50, tanh), FastDense(50, 2))\r\np = initial_params(model)\r\nff(du,u,p,t) = model(u,p)\r\nprob = SecondOrderODEProblem{false}(ff, du0, u0, tspan, p)\r\n\r\nfunction predict(p)\r\n    Array(solve(prob, Tsit5(), p=p, saveat=t))\r\nend\r\n\r\ncorrect_pos = Float32.(transpose(hcat(collect(0:0.05:1)[2:end], collect(2:-0.05:1)[2:end])))\r\n\r\nfunction loss_n_ode(p)\r\n    pred = predict(p)\r\n    sum(abs2, correct_pos .- pred[1:2, :]), pred\r\nend\r\n\r\ndata = Iterators.repeated((), 1000)\r\nopt = ADAM(0.01)\r\n\r\nl1 = loss_n_ode(p)\r\n\r\ncb = function (p,l,pred)\r\n    println(l)\r\n    l < 0.01\r\nend\r\n\r\nres = DiffEqFlux.sciml_train(loss_n_ode, p, opt, cb=cb, maxiters = 1000)","category":"page"}]
}

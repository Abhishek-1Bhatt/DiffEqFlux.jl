<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>sciml_train · DiffEqFlux.jl</title><link rel="canonical" href="https://diffeqflux.sciml.ai/stable/Scimltrain/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">DiffEqFlux.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../examples/LV-ODE/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../examples/LV-Flux/">Lotka-Volterra with Flux.train!</a></li><li><a class="tocitem" href="../examples/LV-delay/">Delay Differential Equations</a></li><li><a class="tocitem" href="../examples/LV-stochastic/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../examples/NN-ODE/">Neural Ordinary Differential Equations</a></li><li><a class="tocitem" href="../examples/NeuralODE_Flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../examples/NN-SDE/">Neural Stochastic Differential Equations</a></li><li><a class="tocitem" href="../examples/LV-Univ/">Universal Differential Equations for Neural Optimal Control</a></li><li><a class="tocitem" href="../examples/LV-NN-Stiff/">Enforcing Physical Constraints with Singular Mass Matrices</a></li><li><a class="tocitem" href="../examples/LV-Jump/">Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</a></li></ul></li><li><a class="tocitem" href="../NeuralDELayers/">Neural Differential Equation Layers</a></li><li><a class="tocitem" href="../Flux/">Use with Flux Chain and train!</a></li><li><a class="tocitem" href="../FastChain/">FastChain</a></li><li><a class="tocitem" href="../GPUs/">GPUs</a></li><li class="is-active"><a class="tocitem" href>sciml_train</a><ul class="internal"><li><a class="tocitem" href="#API-1"><span>API</span></a></li><li><a class="tocitem" href="#Loss-Functions-and-Callbacks-1"><span>Loss Functions and Callbacks</span></a></li></ul></li><li><a class="tocitem" href="../Benchmark/">Benchmark</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>sciml_train</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>sciml_train</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqFlux.jl/blob/master/docs/src/Scimltrain.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sciml_train-1"><a class="docs-heading-anchor" href="#sciml_train-1">sciml_train</a><a class="docs-heading-anchor-permalink" href="#sciml_train-1" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"></div></div><p><code>sciml_train</code> is planned to be replaced by <a href="https://github.com/SciML/GalacticOptim.jl">GalacticOptim.jl</a>   when it is ready. This optimizer library will have a feature superset of <code>sciml_train</code>   but will have a slightly different interface to allow for backprpogation over   the optimization and handling constrained optimization in a nicer manner. Translation   from <code>sciml_train</code> to GalacticOptim&#39;s style will be fairly trivial since the internals   are largely the same, and deprecation warnings will help you update when the time   comes, so do not worry about using this functionalty.</p><p><code>sciml_train</code> is a multi-package optimization setup. It currently allows for using the following nonlinear optimization packages as the backend:</p><ul><li><a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl</a></li><li><a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a></li><li><a href="https://github.com/robertfeldt/BlackBoxOptim.jl">BlackBoxOptim.jl</a></li><li><a href="https://github.com/JuliaOpt/NLopt.jl">NLopt.jl</a></li><li><a href="https://github.com/tpapp/MultistartOptimization.jl">MultistartOptimization.jl</a></li><li><a href="https://github.com/wildart/Evolutionary.jl">Evolutionary.jl</a></li></ul><p>Thus it allows for local and global optimization, both derivative-based and derivative-free, with first and second order methods, in an easy and optimized fashion over the scientific machine learning layer functions provided by DiffEqFlux.jl. These functions come complete with integration with automatic differentiation to allow for ease of use with first and second order optimization methods, where Hessians are derived via forward-over-reverse second order sensitivity analysis.</p><p>To use an optimizer from any of these libraries, one must <code>using</code> the appropriate library first.</p><h2 id="API-1"><a class="docs-heading-anchor" href="#API-1">API</a><a class="docs-heading-anchor-permalink" href="#API-1" title="Permalink"></a></h2><h3 id="Unconstrained-Optimization-1"><a class="docs-heading-anchor" href="#Unconstrained-Optimization-1">Unconstrained Optimization</a><a class="docs-heading-anchor-permalink" href="#Unconstrained-Optimization-1" title="Permalink"></a></h3><pre><code class="language-julia">function sciml_train(loss, _θ, opt, _data = DEFAULT_DATA;
                     cb = (args...) -&gt; false,
                     maxiters = get_maxiters(data),
                     progress=true, save_best=true)</code></pre><h3 id="Box-Constrained-Optimization-1"><a class="docs-heading-anchor" href="#Box-Constrained-Optimization-1">Box Constrained Optimization</a><a class="docs-heading-anchor-permalink" href="#Box-Constrained-Optimization-1" title="Permalink"></a></h3><pre><code class="language-julia">function sciml_train(loss, θ, opt,
                     data = DEFAULT_DATA;
                     lower_bounds, upper_bounds,
                     cb = (args...) -&gt; (false), maxiters = get_maxiters(data))</code></pre><h2 id="Loss-Functions-and-Callbacks-1"><a class="docs-heading-anchor" href="#Loss-Functions-and-Callbacks-1">Loss Functions and Callbacks</a><a class="docs-heading-anchor-permalink" href="#Loss-Functions-and-Callbacks-1" title="Permalink"></a></h2><p>Loss functions in <code>sciml_train</code> treat the first returned value as the return. For example, if one returns <code>(1.0,[2.0])</code>, then the value the optimizer will see is <code>1.0</code>. The other values are passed to the callback function. The callback function is <code>cb(p,args...)</code> where the arguments are the extra returns from the loss. This allows for reusing instead of recalculating. The callback function must return a boolean where if <code>true</code> then the optimizer will prematurely end the optimization. It is called after every successful step, something that is defined in an optimizer-dependent manner.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../GPUs/">« GPUs</a><a class="docs-footer-nextpage" href="../Benchmark/">Benchmark »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 12 May 2020 16:54">Tuesday 12 May 2020</span>. Using Julia version 1.4.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

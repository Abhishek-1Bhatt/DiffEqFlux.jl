<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Augmented Neural Ordinary Differential Equations · DiffEqFlux.jl</title><link rel="canonical" href="https://diffeqflux.sciml.ai/stable/examples/augmented_neural_ode/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DiffEqFlux.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DiffEqFlux.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../lotka_volterra/">Lotka-Volterra with Flux.train!</a></li><li><a class="tocitem" href="../delay_diffeq/">Delay Differential Equations</a></li><li><a class="tocitem" href="../neural_ode_sciml/">Neural Ordinary Differential Equations with sciml_train</a></li><li><a class="tocitem" href="../neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li class="is-active"><a class="tocitem" href>Augmented Neural Ordinary Differential Equations</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Step-by-Step-Explaination-1"><span>Step-by-Step Explaination</span></a></li><li><a class="tocitem" href="#Loading-required-packages-1"><span>Loading required packages</span></a></li><li><a class="tocitem" href="#Generating-a-toy-dataset-1"><span>Generating a toy dataset</span></a></li><li><a class="tocitem" href="#Models-1"><span>Models</span></a></li><li><a class="tocitem" href="#Plotting-the-Results-1"><span>Plotting the Results</span></a></li><li><a class="tocitem" href="#Training-Parameters-1"><span>Training Parameters</span></a></li><li><a class="tocitem" href="#Training-the-Neural-ODE-1"><span>Training the Neural ODE</span></a></li><li><a class="tocitem" href="#Training-the-Augmented-Neural-ODE-1"><span>Training the Augmented Neural ODE</span></a></li><li class="toplevel"><a class="tocitem" href="#Expected-Output-1"><span>Expected Output</span></a></li><li class="toplevel"><a class="tocitem" href="#References-1"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../normalizing_flows/">Continuous Normalizing Flows with sciml_train</a></li><li><a class="tocitem" href="../local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../neural_sde/">Neural Stochastic Differential Equations</a></li><li><a class="tocitem" href="../optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li><li><a class="tocitem" href="../physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li><li><a class="tocitem" href="../second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../jump/">Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</a></li><li><a class="tocitem" href="../universal_diffeq/">Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples</a></li><li><a class="tocitem" href="../minibatch/">Training a Neural Ordinary Differntial Equation with Mini-Batching</a></li></ul></li><li><a class="tocitem" href="../../basis_docs/">Basis</a></li><li><a class="tocitem" href="../../tensor_layer/">Tensor Product Layer</a></li><li><a class="tocitem" href="../../CNFLayer/">Continuous Normalizing Flows Layer</a></li><li><a class="tocitem" href="../../NeuralDELayers/">Neural Differential Equation Layers</a></li><li><a class="tocitem" href="../../Flux/">Use with Flux Chain and train!</a></li><li><a class="tocitem" href="../../FastChain/">FastChain</a></li><li><a class="tocitem" href="../../GPUs/">GPUs</a></li><li><a class="tocitem" href="../../Scimltrain/">sciml_train</a></li><li><a class="tocitem" href="../../Benchmark/">Benchmark</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Augmented Neural Ordinary Differential Equations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Augmented Neural Ordinary Differential Equations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqFlux.jl/blob/master/docs/src/examples/augmented_neural_ode.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Augmented-Neural-Ordinary-Differential-Equations-1"><a class="docs-heading-anchor" href="#Augmented-Neural-Ordinary-Differential-Equations-1">Augmented Neural Ordinary Differential Equations</a><a class="docs-heading-anchor-permalink" href="#Augmented-Neural-Ordinary-Differential-Equations-1" title="Permalink"></a></h1><pre><code class="language-julia">using Flux, DiffEqFlux, DifferentialEquations
using Statistics, LinearAlgebra, Plots
import Flux.Data: DataLoader

function random_point_in_sphere(dim, min_radius, max_radius)
    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius
    direction = randn(dim)
    unit_direction = direction ./ norm(direction)
    return distance .* unit_direction
end

function concentric_sphere(dim, inner_radius_range, outer_radius_range,
                           num_samples_inner, num_samples_outer; batch_size = 64)
    data = []
    labels = []
    for _ in 1:num_samples_inner
        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))
        push!(labels, ones(1, 1))
    end
    for _ in 1:num_samples_outer
        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))
        push!(labels, -ones(1, 1))
    end
    data = cat(data..., dims=2)
    labels = cat(labels..., dims=2)
    return DataLoader(data, labels; batchsize=batch_size, shuffle=true, partial=false)
end

diffeqarray_to_array(x) = reshape(Array(x), size(x)[1:2])

function construct_model(out_dim, input_dim, hidden_dim, augment_dim)
    input_dim = input_dim + augment_dim
    node = NeuralODE(FastChain(FastDense(input_dim, hidden_dim, relu),
                               FastDense(hidden_dim, hidden_dim, relu),
                               FastDense(hidden_dim, input_dim)),
                     (0.f0, 1.f0), Tsit5(), save_everystep = false,
                     reltol = 1e-3, abstol = 1e-3, save_start = false)
    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)
    return Chain((x, p=node.p) -&gt; node(x, p),
                 diffeqarray_to_array,
                 Dense(input_dim, out_dim)), node.p
end

function plot_contour(model, npoints = 300)
    grid_points = zeros(2, npoints ^ 2)
    idx = 1
    x = range(-4.0, 4.0, length = npoints)
    y = range(-4.0, 4.0, length = npoints)
    for x1 in x, x2 in y
        grid_points[:, idx] .= [x1, x2]
        idx += 1
    end
    sol = reshape(model(grid_points), npoints, npoints)
    
    return contour(x, y, sol, fill = true, linewidth=0.0)
end

loss_node(x, y) = mean((model(x) .- y) .^ 2)

println(&quot;Generating Dataset&quot;)

dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)

cb = function()
    global iter += 1
    if iter % 10 == 0
        println(&quot;Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))&quot;)
    end
end

model, parameters = construct_model(1, 2, 64, 0)
opt = ADAM(0.005)
iter = 0

println(&quot;Training Neural ODE&quot;)

for _ in 1:20
    Flux.train!(loss_node, Flux.Params([parameters]), dataloader, opt, cb = cb)
end

plt_node = plot_contour(model)

model, parameters = construct_model(1, 2, 64, 1)
opt = ADAM(0.005)
iter = 0

println()
println(&quot;Training Augmented Neural ODE&quot;)

for _ in 1:20
    Flux.train!(loss_node, Flux.Params([parameters]), dataloader, opt, cb = cb)
end

plt_anode = plot_contour(model)</code></pre><h1 id="Step-by-Step-Explaination-1"><a class="docs-heading-anchor" href="#Step-by-Step-Explaination-1">Step-by-Step Explaination</a><a class="docs-heading-anchor-permalink" href="#Step-by-Step-Explaination-1" title="Permalink"></a></h1><h2 id="Loading-required-packages-1"><a class="docs-heading-anchor" href="#Loading-required-packages-1">Loading required packages</a><a class="docs-heading-anchor-permalink" href="#Loading-required-packages-1" title="Permalink"></a></h2><pre><code class="language-julia">using Flux, DiffEqFlux, DifferentialEquations
using Statistics, LinearAlgebra, Plots
import Flux.Data: DataLoader</code></pre><h2 id="Generating-a-toy-dataset-1"><a class="docs-heading-anchor" href="#Generating-a-toy-dataset-1">Generating a toy dataset</a><a class="docs-heading-anchor-permalink" href="#Generating-a-toy-dataset-1" title="Permalink"></a></h2><p>In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign <code>1</code> to any point which lies inside the inner circle, and <code>-1</code> to any point which lies between the inner and outer circle. Our first function <code>random_point_in_sphere</code> samples points uniformly between 2 concentric circles/spheres of radii <code>min_radius</code> and <code>max_radius</code> respectively.</p><pre><code class="language-julia">function random_point_in_sphere(dim, min_radius, max_radius)
    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius
    direction = randn(dim)
    unit_direction = direction ./ norm(direction)
    return distance .* unit_direction
end</code></pre><p>Next, we will construct a dataset of these points and use Flux&#39;s DataLoader to automatically minibatch and shuffle the data.</p><pre><code class="language-julia">function concentric_sphere(dim, inner_radius_range, outer_radius_range,
                           num_samples_inner, num_samples_outer; batch_size = 64)
    data = []
    labels = []
    for _ in 1:num_samples_inner
        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))
        push!(labels, ones(1, 1))
    end
    for _ in 1:num_samples_outer
        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))
        push!(labels, -ones(1, 1))
    end
    data = cat(data..., dims=2)
    labels = cat(labels..., dims=2)
    return DataLoader(data, labels; batchsize=batch_size, shuffle=true, partial=false)
end</code></pre><h2 id="Models-1"><a class="docs-heading-anchor" href="#Models-1">Models</a><a class="docs-heading-anchor-permalink" href="#Models-1" title="Permalink"></a></h2><p>We consider 2 models in this tuturial. The first is a simple Neural ODE which is described in detail in <a href="https://diffeqflux.sciml.ai/dev/examples/neural_ode_sciml/">this tutorial</a>. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size <code>size(x, 1) + augment_dim</code> instead of <code>size(x, 1)</code> and construct that layer accordingly.</p><pre><code class="language-julia">diffeqarray_to_array(x) = reshape(Array(x), size(x)[1:2])

function construct_model(out_dim, input_dim, hidden_dim, augment_dim)
    input_dim = input_dim + augment_dim
    node = NeuralODE(FastChain(FastDense(input_dim, hidden_dim, relu),
                               FastDense(hidden_dim, hidden_dim, relu),
                               FastDense(hidden_dim, input_dim)),
                     (0.f0, 1.f0), Tsit5(), save_everystep = false,
                     reltol = 1e-3, abstol = 1e-3, save_start = false)
    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)
    return Chain((x, p=node.p) -&gt; node(x, p),
                 diffeqarray_to_array,
                 Dense(input_dim, out_dim)), node.p
end</code></pre><h2 id="Plotting-the-Results-1"><a class="docs-heading-anchor" href="#Plotting-the-Results-1">Plotting the Results</a><a class="docs-heading-anchor-permalink" href="#Plotting-the-Results-1" title="Permalink"></a></h2><p>Here, we define an utility to plot our model regression results as a heatmap.</p><pre><code class="language-julia">function plot_contour(model, npoints = 300)
    grid_points = zeros(2, npoints ^ 2)
    idx = 1
    x = range(-4.0, 4.0, length = npoints)
    y = range(-4.0, 4.0, length = npoints)
    for x1 in x, x2 in y
        grid_points[:, idx] .= [x1, x2]
        idx += 1
    end
    sol = reshape(model(grid_points), npoints, npoints)
    
    return contour(x, y, sol, fill = true, linewidth=0.0)
end</code></pre><h2 id="Training-Parameters-1"><a class="docs-heading-anchor" href="#Training-Parameters-1">Training Parameters</a><a class="docs-heading-anchor-permalink" href="#Training-Parameters-1" title="Permalink"></a></h2><h3 id="Loss-Functions-1"><a class="docs-heading-anchor" href="#Loss-Functions-1">Loss Functions</a><a class="docs-heading-anchor-permalink" href="#Loss-Functions-1" title="Permalink"></a></h3><p>We use the L2 distance between the model prediction <code>model(x)</code> and the actual prediction <code>y</code> as the optimization objective.</p><pre><code class="language-julia">loss_node(x, y) = mean((model(x) .- y) .^ 2)</code></pre><h3 id="Dataset-1"><a class="docs-heading-anchor" href="#Dataset-1">Dataset</a><a class="docs-heading-anchor-permalink" href="#Dataset-1" title="Permalink"></a></h3><p>Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of <code>4000</code> data points.</p><pre><code class="language-julia">dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)</code></pre><h3 id="Callback-Function-1"><a class="docs-heading-anchor" href="#Callback-Function-1">Callback Function</a><a class="docs-heading-anchor-permalink" href="#Callback-Function-1" title="Permalink"></a></h3><p>Additionally we define a callback function which displays the total loss at specific intervals.</p><pre><code class="language-julia">cb = function()
    global iter += 1
    if iter % 10 == 1
        println(&quot;Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))&quot;)
    end
end</code></pre><h3 id="Optimizer-1"><a class="docs-heading-anchor" href="#Optimizer-1">Optimizer</a><a class="docs-heading-anchor-permalink" href="#Optimizer-1" title="Permalink"></a></h3><p>We use ADAM as the optimizer with a learning rate of 0.005</p><pre><code class="language-julia">opt = ADAM(0.005)</code></pre><h2 id="Training-the-Neural-ODE-1"><a class="docs-heading-anchor" href="#Training-the-Neural-ODE-1">Training the Neural ODE</a><a class="docs-heading-anchor-permalink" href="#Training-the-Neural-ODE-1" title="Permalink"></a></h2><p>To train our neural ode model, we need to pass the appropriate learnable parameters, <code>parameters</code> which is returned by the <code>construct_models</code> function. It is simply the <code>node.p</code> vector. We then train our model for <code>20</code> epochs.</p><pre><code class="language-julia">model, parameters = construct_model(1, 2, 64, 0)

for _ in 1:20
    Flux.train!(loss_node, Flux.Params([parameters]), dataloader, opt, cb = cb)
end</code></pre><p>Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.</p><p><img src="https://user-images.githubusercontent.com/30564094/85356368-6d96a880-b52c-11ea-8f21-6f35df0d5c8c.png" alt="node"/></p><h2 id="Training-the-Augmented-Neural-ODE-1"><a class="docs-heading-anchor" href="#Training-the-Augmented-Neural-ODE-1">Training the Augmented Neural ODE</a><a class="docs-heading-anchor-permalink" href="#Training-the-Augmented-Neural-ODE-1" title="Permalink"></a></h2><p>Our training configuration will be same as that of Neural ODE. Only in this case we have augmented the input with a single zero. This makes the problem 3 dimensional and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs please refer to [1].</p><pre><code class="language-julia">model, parameters = construct_model(1, 2, 64, 1)

for _ in 1:20
    Flux.train!(loss_node, Flux.Params([parameters]), dataloader, opt, cb = cb)
end</code></pre><p>For the augmented Neural ODE we notice that the artifact is gone.</p><p><img src="https://user-images.githubusercontent.com/30564094/85356373-6f606c00-b52c-11ea-8431-fb74ff01dde5.png" alt="anode"/></p><h1 id="Expected-Output-1"><a class="docs-heading-anchor" href="#Expected-Output-1">Expected Output</a><a class="docs-heading-anchor-permalink" href="#Expected-Output-1" title="Permalink"></a></h1><pre><code class="language-julia">Generating Dataset
Training Neural ODE
Iteration 10 || Loss = 1.120950346042287
Iteration 20 || Loss = 0.7532827576978123
Iteration 30 || Loss = 0.6553072657563143
Iteration 40 || Loss = 0.5776224441286926
Iteration 50 || Loss = 0.520220032887143
Iteration 60 || Loss = 0.4720985558909219
Iteration 70 || Loss = 0.40864928280982776
Iteration 80 || Loss = 0.31852414526416173
Iteration 90 || Loss = 0.2481226283169072
Iteration 100 || Loss = 0.18494126827346924
Iteration 110 || Loss = 0.1470610323283855
Iteration 120 || Loss = 0.11047625817186725
Iteration 130 || Loss = 0.08345815290524315
Iteration 140 || Loss = 0.07403708346807107
Iteration 150 || Loss = 0.07533284314926621
Iteration 160 || Loss = 0.0807687251841976
Iteration 170 || Loss = 0.07462567382624537
Iteration 180 || Loss = 0.07620629179132667
Iteration 190 || Loss = 0.053866679112776615
Iteration 200 || Loss = 0.04878556792331758
Iteration 210 || Loss = 0.04951108534243689
Iteration 220 || Loss = 0.05219504247382631
Iteration 230 || Loss = 0.04829135367843049
Iteration 240 || Loss = 0.049767843225925146
Iteration 250 || Loss = 0.053435584980049196
Iteration 260 || Loss = 0.06330034970941116
Iteration 270 || Loss = 0.0674349462469368
Iteration 280 || Loss = 0.054646060311423855
Iteration 290 || Loss = 0.058587195675343276

Training Augmented Neural ODE
Iteration 10 || Loss = 0.8052180670411724
Iteration 20 || Loss = 0.6493323407007557
Iteration 30 || Loss = 0.43219055806073575
Iteration 40 || Loss = 0.17996087680619746
Iteration 50 || Loss = 0.10677325033978695
Iteration 60 || Loss = 0.06763209051354825
Iteration 70 || Loss = 0.05557440341534089
Iteration 80 || Loss = 0.05175036274021895
Iteration 90 || Loss = 0.04598351412837163
Iteration 100 || Loss = 0.043676742887634665
Iteration 110 || Loss = 0.03984149571984987
Iteration 120 || Loss = 0.0377291626995965
Iteration 130 || Loss = 0.03618670060916931
Iteration 140 || Loss = 0.034320209919086186
Iteration 150 || Loss = 0.0336497946533457
Iteration 160 || Loss = 0.031927051142359046
Iteration 170 || Loss = 0.029903758740715262
Iteration 180 || Loss = 0.027844496982240865
Iteration 190 || Loss = 0.026784325989841085
Iteration 200 || Loss = 0.024220633742520606
Iteration 210 || Loss = 0.022410493158517744
Iteration 220 || Loss = 0.021468741537086423
Iteration 230 || Loss = 0.022414650405633993
Iteration 240 || Loss = 0.018358236240712273
Iteration 250 || Loss = 0.01700812985852846
Iteration 260 || Loss = 0.01573442645516374
Iteration 270 || Loss = 0.014445097490699855
Iteration 280 || Loss = 0.014170123422892231
Iteration 290 || Loss = 0.01250998329088748</code></pre><h1 id="References-1"><a class="docs-heading-anchor" href="#References-1">References</a><a class="docs-heading-anchor-permalink" href="#References-1" title="Permalink"></a></h1><p>[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. &quot;Augmented neural odes.&quot; Advances in Neural Information Processing Systems. 2019.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mnist_neural_ode/">« GPU-based MNIST Neural ODE Classifier</a><a class="docs-footer-nextpage" href="../normalizing_flows/">Continuous Normalizing Flows with sciml_train »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 27 June 2020 01:36">Saturday 27 June 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Controlling Choices of Adjoints · DiffEqFlux.jl</title><link rel="canonical" href="https://diffeqflux.sciml.ai/stable/ControllingAdjoints/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="DiffEqFlux.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DiffEqFlux.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../examples/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../examples/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../examples/lotka_volterra/">Lotka-Volterra with Flux.train!</a></li><li><a class="tocitem" href="../examples/neural_ode_sciml/">Neural Ordinary Differential Equations with sciml_train</a></li><li><a class="tocitem" href="../examples/neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../examples/mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../examples/delay_diffeq/">Delay Differential Equations</a></li><li><a class="tocitem" href="../examples/augmented_neural_ode/">Augmented Neural Ordinary Differential Equations</a></li><li><a class="tocitem" href="../examples/normalizing_flows/">Continuous Normalizing Flows with sciml_train</a></li><li><a class="tocitem" href="../examples/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../examples/neural_sde/">Neural Stochastic Differential Equations</a></li><li><a class="tocitem" href="../examples/collocation/">Smoothed Collocation for Fast Two-Stage Training</a></li><li><a class="tocitem" href="../examples/pde_constrained/">Partial Differential Equation Constrained Optimization</a></li><li><a class="tocitem" href="../examples/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../examples/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../examples/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li><li><a class="tocitem" href="../examples/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li><li><a class="tocitem" href="../examples/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../examples/jump/">Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)</a></li><li><a class="tocitem" href="../examples/universal_diffeq/">Universal Ordinary, Stochastic, and Partial Diffrential Equation Examples</a></li><li><a class="tocitem" href="../examples/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li><li><a class="tocitem" href="../examples/tensor_layer/">Physics Informed Machine Learning with TensorLayer</a></li><li><a class="tocitem" href="../examples/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../examples/hamiltonian_nn/">Hamiltonian Neural Network</a></li></ul></li><li><span class="tocitem">Layers</span><ul><li><a class="tocitem" href="../layers/BasisLayers/">Classical Basis Layers</a></li><li><a class="tocitem" href="../layers/TensorLayer/">Tensor Product Layer</a></li><li><a class="tocitem" href="../layers/CNFLayer/">Continuous Normalizing Flows Layer</a></li><li><a class="tocitem" href="../layers/SplineLayer/">Spline Layer</a></li><li><a class="tocitem" href="../layers/NeuralDELayers/">Neural Differential Equation Layers</a></li><li><a class="tocitem" href="../layers/HamiltonianNN/">Hamiltonian Neural Network Layer</a></li></ul></li><li class="is-active"><a class="tocitem" href>Controlling Choices of Adjoints</a><ul class="internal"><li><a class="tocitem" href="#Choosing-Sensitivity-Analysis-Methods-1"><span>Choosing Sensitivity Analysis Methods</span></a></li><li><a class="tocitem" href="#Choices-of-Vector-Jacobian-Products-(autojacvec)-1"><span>Choices of Vector-Jacobian Products (autojacvec)</span></a></li><li><a class="tocitem" href="#Manual-VJPs-1"><span>Manual VJPs</span></a></li><li><a class="tocitem" href="#Optimize-then-Discretize-1"><span>Optimize-then-Discretize</span></a></li><li><a class="tocitem" href="#Discretize-then-Optimize-1"><span>Discretize-then-Optimize</span></a></li><li class="toplevel"><a class="tocitem" href="#Special-Notes-on-Equation-Types-1"><span>Special Notes on Equation Types</span></a></li><li><a class="tocitem" href="#Differential-Algebraic-Equations-1"><span>Differential-Algebraic Equations</span></a></li><li><a class="tocitem" href="#Stochastic-Differential-Equations-1"><span>Stochastic Differential Equations</span></a></li><li><a class="tocitem" href="#Delay-Differential-Equations-1"><span>Delay Differential Equations</span></a></li></ul></li><li><a class="tocitem" href="../Flux/">Use with Flux Chain and train!</a></li><li><a class="tocitem" href="../FastChain/">FastChain</a></li><li><a class="tocitem" href="../Collocation/">Smoothed Collocation</a></li><li><a class="tocitem" href="../GPUs/">GPUs</a></li><li><a class="tocitem" href="../Scimltrain/">sciml_train</a></li><li><a class="tocitem" href="../Benchmark/">Benchmark</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Controlling Choices of Adjoints</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Controlling Choices of Adjoints</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqFlux.jl/blob/master/docs/src/ControllingAdjoints.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="adjoints-1"><a class="docs-heading-anchor" href="#adjoints-1">Controlling Choices of Adjoints</a><a class="docs-heading-anchor-permalink" href="#adjoints-1" title="Permalink"></a></h1><p>DiffEqFlux is capable of training neural networks embedded inside of differential equations with many different techniques. For all of the details, see the <a href="https://diffeq.sciml.ai/latest/analysis/sensitivity/">DifferentialEquations.jl local sensitivity analysis</a> documentation. Here we will summarize these methodologies in the context of neural differential equations and scientific machine learning.</p><h2 id="Choosing-Sensitivity-Analysis-Methods-1"><a class="docs-heading-anchor" href="#Choosing-Sensitivity-Analysis-Methods-1">Choosing Sensitivity Analysis Methods</a><a class="docs-heading-anchor-permalink" href="#Choosing-Sensitivity-Analysis-Methods-1" title="Permalink"></a></h2><p>A sensitivity analysis method can be passed to a solver via the <code>sensealg</code> keyword argument. For example:</p><pre><code class="language-julia">solve(prob,Tsit5(),sensealg=BacksolveAdjoint(autojacvec=ZygoteVJP()))</code></pre><p>sets the adjoint sensitivity analysis so that, when this call is encountered in the gradient calculation of any of the Julia reverse-mode AD frameworks, the differentiation will be replaced with the <code>BacksolveAdjoint</code> method where internal vector-Jacobian products are performed using Zygote.jl. From the <a href="https://diffeq.sciml.ai/latest/analysis/sensitivity/">DifferentialEquations.jl local sensitivity analysis</a> page, we note that the following choices for <code>sensealg</code> exist:</p><ul><li><code>BacksolveAdjoint</code></li><li><code>InterpolatingAdjoint</code> (with checkpoints)</li><li><code>QuadratureAdjoint</code></li><li><code>TrackerAdjoint</code></li><li><code>ReverseDiffAdjoint</code> (currently requires <code>using DistributionsAD</code>)</li><li><code>ZygoteAdjoint</code> (currently limited to special solvers)</li></ul><p>Additionally, there are methodologies for forward sensitivity analysis:</p><ul><li><code>ForwardSensitivty</code></li><li><code>ForwardDiffSensitivty</code></li></ul><p>These methods have very low overhead compared to adjoint methods but have poor scaling with respect to increased numbers of parameters. <a href="https://arxiv.org/abs/1812.01892">Our benchmarks demonstrate a cutoff of around 100 parameters</a>, where for models with less than 100 parameters these techniques are more efficient, but when there are more than 100 parameters (like in neural ODEs) these methods are less efficient than the adjoint methods.</p><h2 id="Choices-of-Vector-Jacobian-Products-(autojacvec)-1"><a class="docs-heading-anchor" href="#Choices-of-Vector-Jacobian-Products-(autojacvec)-1">Choices of Vector-Jacobian Products (autojacvec)</a><a class="docs-heading-anchor-permalink" href="#Choices-of-Vector-Jacobian-Products-(autojacvec)-1" title="Permalink"></a></h2><p>With each of these solvers, <code>autojacvec</code> can be utilized to choose how the internal vector-Jacobian products of the <code>f</code> function are computed. The choices are:</p><ul><li><code>ReverseDiffVJP(compile::Bool)</code></li><li><code>TrackerVJP</code></li><li><code>ZygoteVJP</code></li><li><code>nothing</code> (a default choice given characteristics of the types in your model)</li><li><code>true</code> (Forward-mode AD Jacobian-vector products)</li><li><code>false</code> (Numerical Jacobian-vector products)</li></ul><p>As a quick summary of the VJP choices, <code>ReverseDiffVJP</code> is usually the fastest when scalarized operations exist in the <code>f</code> function (like in scientific machine learning applications like Universal Differential Equations) and the boolean <code>compile</code> (i.e. <code>ReverseDiffVJP(true)</code>) is the absolute fastest but requires that the <code>f</code> function of the ODE/DAE/SDE/DDE has no branching. However, if the ODE/DAE/SDE/DDE is written with mostly vectorized functions (like neural networks and other layers from <a href="https://fluxml.ai/">Flux.jl</a>), then <code>ZygoteVJP</code> tends to be the fastest VJP method. Note that <code>ReverseDiffVJP</code> does not support GPUs, while <code>TrackerAdjoint</code> is not as efficient but supports GPUs. As other vector-Jacobian product systems become available in Julia they will be added to this system so that no user code changes are required to interface with these methodologies. Forward-mode AD should only be used on sufficiently small equations, and numerical autojacvecs should only be used if the <code>f</code> function is not differentiable (i.e. is a Fortran code).</p><h2 id="Manual-VJPs-1"><a class="docs-heading-anchor" href="#Manual-VJPs-1">Manual VJPs</a><a class="docs-heading-anchor-permalink" href="#Manual-VJPs-1" title="Permalink"></a></h2><p>Note that when defining your differential equation the vjp can be manually overwritten by providing a <code>vjp(u,p,t)</code> that returns a tuple <code>f(u,p,t),v-&gt;J*v</code> in the form of <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/">ChainRules.jl</a>. When this is done, the choice of <code>ZygoteVJP</code> will utilize your VJP function during the internal steps of the adjoint. This is useful for models where automatic differentiation may have trouble producing optimal code. This can be paired with <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a> for producing hyper-optimized, sparse, and parallel VJP functions utilizing the automated symbolic conversions.</p><h2 id="Optimize-then-Discretize-1"><a class="docs-heading-anchor" href="#Optimize-then-Discretize-1">Optimize-then-Discretize</a><a class="docs-heading-anchor-permalink" href="#Optimize-then-Discretize-1" title="Permalink"></a></h2><p><a href="https://arxiv.org/abs/1806.07366">The original neural ODE paper</a> popularized optimize-then-discretize with O(1) adjoints via backsolve. This is the methodology <code>BacksolveAdjoint</code> When training non-stiff neural ODEs, <code>BacksolveAdjoint</code> with <code>ZygoteVJP</code> is generally the fastest method. Additionally, this method does not require storing the values of any intermediate points and is thus the most memory efficient. However, <code>BacksolveAdjoint</code> is prone to instabilities whenever the Lipschitz constant is sufficiently large, like in stiff equations, PDE discretizations, and many other contexts, so it is not used by default. When training a neural ODE for machine learning applications, the user should try <code>BacksolveAdjoint</code> and see if it is sufficiently accurate on their problem.</p><p>Note that DiffEqFlux&#39;s implementation of <code>BacksolveAdjoint</code> includes an extra feature <code>BacksolveAdjoint(checkpointing=true)</code> which mixes checkpointing with <code>BacksolveAdjoint</code>. What this method does is that, at <code>saveat</code> points, values from the forward pass are saved. Since the reverse solve should numerically be the same as the forward pass, issues with divergence of the reverse pass are mitigated by restarting the reverse pass at the <code>saveat</code> value from the forward pass. This reduces the divergence and can lead to better gradients at the cost of higher memory usage due to having to save some values of the forward pass. This can stabilize the adjoint in some applications, but for highly stiff applications the divergence can be too fast for this to work in practice.</p><p>To avoid the issues of backwards solving the ODE, <code>InterpolatingAdjoint</code> and <code>QuadratureAdjoint</code> utilize information from the forward pass. By default these methods utilize the <a href="https://diffeq.sciml.ai/latest/basics/solution/#Interpolations-1">continuous solution</a> provided by DifferentialEquations.jl in the calculations of the adjoint pass. <code>QuadratureAdjoint</code> uses this to build a continuous function for the solution of adjoint equation and then performs an adaptive quadrature via <a href="https://github.com/SciML/Quadrature.jl">Quadrature.jl</a>, while <code>InterpolatingAdjoint</code> appends the integrand to the ODE so it&#39;s computed simultaneously to the Legrange multiplier. When memory is not an issue, we find that the <code>QuadratureAdjoint</code> approach tends to be the most efficient as it has a significantly smaller adjoint differential equation and the quadrature converges very fast, but this form requires holding the full continuous solution of the adjoint which can be a significant burden for large parameter problems. The <code>InterpolatingAdjoint</code> is thus a compromise between memory efficiency and compute efficiency, and is in the same spirit as <a href="https://computing.llnl.gov/projects/sundials">CVODES</a>.</p><p>However, if the memory cost of the <code>InterpolatingAdjoint</code> is too high, checkpointing can be used via <code>InterpolatingAdjoint(checkpointing=true)</code>. When this is used, the checkpoints default to <code>sol.t</code> of the forward pass (i.e. the saved timepoints usually set by <code>saveat</code>). Then in the adjoint, intervals of <code>sol.t[i-1]</code> to <code>sol.t[i]</code> are re-solved in order to obtain a short interpolation which can be utilized in the adjoints. This at most results in two full solves of the forward pass, but dramatically reduces the computational cost while being a low-memory format. This is the preferred method for highly stiff equations when memory is an issue, i.e. stiff PDEs or large neural DAEs.</p><p>For forward-mode, the <code>ForwardSensitivty</code> is the version that performs the optimize-then-discretize approach. In this case, <code>autojacvec</code> corresponds to the method for computing <code>J*v</code> within the forward sensitivity equations, which is either <code>true</code> or <code>false</code> for whether to use Jacobian-free forward-mode AD (via ForwardDiff.jl) or Jacobian-free numerical differentiation.</p><h2 id="Discretize-then-Optimize-1"><a class="docs-heading-anchor" href="#Discretize-then-Optimize-1">Discretize-then-Optimize</a><a class="docs-heading-anchor-permalink" href="#Discretize-then-Optimize-1" title="Permalink"></a></h2><p>In this approach the discretization is done first and then optimization is done on the discretized system. While traditionally this can be done discrete sensitivity analysis, this is can be equivalently done by automatic differentiation on the solver itself. <code>ReverseDiffAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a>, <code>ZygoteAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>, and <code>TrackerAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a>. In addition, <code>ForwardDiffSensitivty</code> performs forward-mode automatic differentiation on the solver via <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a>.</p><p>We note that many studies have suggested that <a href="https://arxiv.org/abs/2005.13420">this approach produces more accurate gradients than the optimize-than-discretize approach</a></p><h1 id="Special-Notes-on-Equation-Types-1"><a class="docs-heading-anchor" href="#Special-Notes-on-Equation-Types-1">Special Notes on Equation Types</a><a class="docs-heading-anchor-permalink" href="#Special-Notes-on-Equation-Types-1" title="Permalink"></a></h1><p>While all of the choices are compatible with ordinary differential equations, specific notices apply to other forms:</p><h2 id="Differential-Algebraic-Equations-1"><a class="docs-heading-anchor" href="#Differential-Algebraic-Equations-1">Differential-Algebraic Equations</a><a class="docs-heading-anchor-permalink" href="#Differential-Algebraic-Equations-1" title="Permalink"></a></h2><p>We note that while all 3 are compatible with index-1 DAEs via the <a href="https://arxiv.org/abs/2001.04385">derivation in the universal differential equations paper</a> (note the reinitialization), we do not recommend <code>BacksolveAdjoint</code> one DAEs because the stiffness inherent in these problems tends to cause major difficulties with the accuracy of the backwards solution due to reinitialization of the algebraic variables.</p><h2 id="Stochastic-Differential-Equations-1"><a class="docs-heading-anchor" href="#Stochastic-Differential-Equations-1">Stochastic Differential Equations</a><a class="docs-heading-anchor-permalink" href="#Stochastic-Differential-Equations-1" title="Permalink"></a></h2><p>We note that all of the adjoints except <code>QuadratureAdjoint</code> are applicable to stochastic differential equations.</p><h2 id="Delay-Differential-Equations-1"><a class="docs-heading-anchor" href="#Delay-Differential-Equations-1">Delay Differential Equations</a><a class="docs-heading-anchor-permalink" href="#Delay-Differential-Equations-1" title="Permalink"></a></h2><p>We note that only the discretize-then-optimize methods are applicable to delay differential equations. Constant lag and variable lag delay differential equation parameters can be estimated, but the lag times themselves are unable to be estimated through these automatic differentiation techniques.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../layers/HamiltonianNN/">« Hamiltonian Neural Network Layer</a><a class="docs-footer-nextpage" href="../Flux/">Use with Flux Chain and train! »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 10 August 2020 01:35">Monday 10 August 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
